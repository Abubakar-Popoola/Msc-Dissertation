{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Navigation\n",
    "d = os.getcwd()\n",
    "print(f'Current: {d}')\n",
    "file_location = \"/home/uthlakanyana/Dropbox/Dissertation Code/London Crime\"\n",
    "os.chdir(file_location)\n",
    "\n",
    "# Obtain Directory Items\n",
    "locations = [file_location+\"/\"+str(f) for f in os.listdir()]\n",
    "    \n",
    "# Obtain the Files Locations in Each Directory\n",
    "dataframes = []\n",
    "for l in locations:\n",
    "    os.chdir(l)\n",
    "    dataframes.extend([l+\"/\"+i for i in os.listdir(l)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Data into a Single Dataframe\n",
    "dataframes = [pd.read_csv(i,usecols=[\"Month\",\"Reported by\",\"Longitude\",\n",
    "                                     \"Latitude\",\"Location\",\"LSOA code\", \n",
    "                                     \"LSOA name\", \"Crime type\"]).rename(columns ={\"Reported by\":\"ReportedBy\",\n",
    "                                                                                \"LSOA code\":\"LSOA_Code\",\n",
    "                                                                                \"LSOA name\": \"LSOA_Name\"}) for i in dataframes]\n",
    "\n",
    "\n",
    "Combined = pd.concat(dataframes,axis=0)\n",
    "Combined.sort_values(by=\"Month\")\n",
    "Combined.LSOA_Name = Combined.LSOA_Name.fillna(\"ZZZ\")\n",
    "#Combined.to_csv(\"/home/uthlakanyana/Combined.csv\",index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSOA Data\n",
    "# Note that city of London data is marked as confidential\n",
    "lsoa_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/LSOA Data -2021.csv\",\n",
    "                        usecols=[\"Area name\",\"Working households (thousands)\",\n",
    "                                 \"Working households (per cent)\",\n",
    "                                 \"Mixed households (thousands)\",\n",
    "                                 \"Mixed households (per cent)\",\n",
    "                                 \"Workless households (thousands)\",\n",
    "                                 \"Workless households (per cent)\"]).rename(columns={\"Area Code\":\"LSOA_Code\",\n",
    "                                                                                    \"Area name\":\"LSOA_Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and Designate Areas/Boroughs\n",
    "areas = [\"Camden\",\"City of London\",\"Hackney\",\"Hammersmith and Fulham\",\n",
    "         \"Haringey\",\"Islington\",\"Kensington and Chelsea\",\"Lambeth\",\"Lewisham\",\n",
    "         \"Newham\",\"Southwark\",\"Tower Hamlets\",\"Wandsworth\",\"Westminster\",\n",
    "         \"Barking and Dagenham\",\"Barnet\",\"Bexley\",\"Brent\",\"Bromley\",\"Croydon\",\"Ealing\",\n",
    "         \"Enfield\",\"Greenwich\",\"Harrow\",\"Havering\",\"Hillingdon\",\"Hounslow\",\n",
    "         \"Kingston upon Thames\",\"Merton\",\"Redbridge\",\"Richmond upon Thames\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Full Area Names With Basic Versions; Keep Track of the Amount of Crime They Account for as Well\n",
    "x = 0\n",
    "for zone in areas:\n",
    "    Combined.loc[Combined[\"LSOA_Name\"].str.contains(zone),\"LSOA_Name\"] = zone\n",
    "    lsoa_data.loc[lsoa_data[\"LSOA_Name\"].str.contains(zone),\"LSOA_Name\"] = zone\n",
    "    print(zone,len(Combined[Combined[\"LSOA_Name\"]==zone]))\n",
    "    x = x+len(Combined[Combined[\"LSOA_Name\"]==zone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Percentage of Crime under the Umbrella of Proper LSOAs\n",
    "print(x/len(Combined))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Data\n",
    "Comprehensive_LSOA = pd.merge(Combined,lsoa_data,on=\"LSOA_Name\",how=\"left\").fillna(np.nan)     \n",
    "print(len(Comprehensive_LSOA))   \n",
    "Comprehensive_LSOA.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Crime Data and Reorganize It\n",
    "\n",
    "data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data.csv\",low_memory=False)\n",
    "data[\"Month\"] = data[\"Month\"].astype(\"datetime64[ns]\")\n",
    "data = data.sort_values(by=\"Month\").reset_index().drop([\"index\"],axis=1)\n",
    "initial_size = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"Month\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Geographic Data and Join It to the Crime Data\n",
    "# Use the EPSG Relevant to the UK\n",
    "\n",
    "london_geo_data = gpd.read_file(\"/home/uthlakanyana/Dropbox/Dissertation Code/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\").rename(columns={\"LSOA11CD\":\"LSOA_Code\"})\n",
    "london_geo_data = london_geo_data.to_crs(27700)\n",
    "london_geo_data[\"Area\"] = london_geo_data[\"geometry\"].area/10**6\n",
    "\n",
    "data = data.join(london_geo_data.set_index('LSOA_Code'),on=\"LSOA_Code\")\n",
    "data = data.drop([\"ReportedBy\",\"Location\",\"MSOA11CD\",\"MSOA11NM\",\"LAD11CD\",\"LAD11NM\",\"RGN11CD\",\"RGN11NM\",\"geometry\",\"USUALRES\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = data.merge(london_geo_data,on=\"LSOA_Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.LSOA_Code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(london_geo_data.LSOA_Code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_geo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the First Entry\n",
    "\n",
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Missing Data\n",
    "\n",
    "data2 = data[data.Longitude.isna() == False]\n",
    "data2 = data[data.Latitude.isna() == False]\n",
    "after_missing_size = len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Initial Data Remaining\n",
    "\n",
    "print(after_missing_size/initial_size * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City of London LSOA Information Is always Confidential and/or Not Collected\n",
    "\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Working households (thousands)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Working households (per cent)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Working households (per cent)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Mixed households (thousands)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Mixed households (per cent)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Workless households (thousands)\"].unique())\n",
    "print(data2[data2[\"LSOA_Name\"] == \"City of London\"][\"Workless households (per cent)\"].unique())\n",
    "\n",
    "# We Can Confirm that It Contributes Little to the Entirety of the Data Set in Terms of Criminal Instances (About 10%)\n",
    "\n",
    "print(\"City of London contribution is\",data[data[\"LSOA_Name\"]==\"City of London\"].count().sum()/len(data) * 100,\"%\")\n",
    "\n",
    "# Thus We Can Remove It Entirely For Now. It Can Be Treated Separately Later\n",
    "\n",
    "city_of_london = data[data[\"LSOA_Name\"]==\"City of London\"]\n",
    "\n",
    "data3 = data2[data2[\"LSOA_Name\"] != \"City of London\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data to Proper Values\n",
    "\n",
    "print(data3.dtypes)\n",
    "data3 = data3.astype({\"Working households (thousands)\":\"float64\",\n",
    "                      \"Working households (per cent)\":\"float64\",\n",
    "                      \"Mixed households (thousands)\":\"float64\",\n",
    "                      \"Mixed households (per cent)\": \"float64\",\n",
    "                      \"Workless households (thousands)\": \"float64\",\n",
    "                      \"Workless households (per cent)\": \"float64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Relevant Columns Have Been Changed\n",
    "\n",
    "data3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any NaN Values in the dataset\n",
    "\n",
    "data3.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The First Six Belong to the LSOAs not Represented in the Economic LSOA Data. To Confirm:\n",
    "\n",
    "unique_na = data3[data3[\"Working households (thousands)\"].isna() == True][\"LSOA_Name\"].unique()\n",
    "number_na = data3[data3[\"Working households (thousands)\"].isna() == True][\"LSOA_Name\"].count()\n",
    "print(number_na)\n",
    "\n",
    "# They Form a Small Proportion of the Dataset\n",
    "\n",
    "proportion = round(((number_na/len(data3))*100),2)\n",
    "\n",
    "print(f\"{proportion}%\")\n",
    "\n",
    "# We Can Drop Them\n",
    "\n",
    "data4 = data3[data3[\"Working households (thousands)\"].isna() == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There Are Now No NaN Values from the Crime Dataset + LSOA Information\n",
    "\n",
    "data4.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Percentage Reduction in the Dataset Size So Far\n",
    "\n",
    "print(f\"{round((100-((len(data4)/initial_size) * 100)),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some LSOAs in Brent Are Not Recognized in the Geographic Dataset\n",
    "\n",
    "print(data4[data4[\"LSOA11NM\"].isna()].iloc[1])\n",
    "\n",
    "# Check if the First LSOA Code Here Is in the Geographic Dataset\n",
    "data4[data4[\"LSOA11NM\"].isna()].iloc[0][3] in london_geo_data[london_geo_data[\"LAD11NM\"]==\"Brent\"][\"LSOA_Code\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4[\"LSOA11NM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Problematic Columns and their Quantities\n",
    "\n",
    "missing_lsoa11nm = data4[data4[\"LSOA11NM\"].isna()==True].copy()\n",
    "missing_lsoa11nm[\"Instance\"] = 1\n",
    "missing_lsoa11nm[[\"LSOA_Name\",\"Instance\"]].groupby([\"LSOA_Name\"],as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Can Thus Drop the Entries Since They Are Insignificant and Useless\n",
    "\n",
    "data5 = data4[data4[\"LSOA11NM\"].isna() == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If We Check for Null Values in the Dataset, We Can See That There Are None\n",
    "\n",
    "data5.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Graphing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the Dataset is Cleaned. Each Feature Should Be Appropriately Typed, and there Should Be No NaN Values in the Dataset\n",
    "\n",
    "data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 2.csv\")\n",
    "data[\"Month\"] = data[\"Month\"].astype(\"datetime64[ns]\")\n",
    "#print(data.dtypes)\n",
    "#data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Month\"].dt.year==2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bar Chart of Crime Incidence By Location\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "#plt.ticklabel_format(style = 'plain')\n",
    "sns.set(rc={'figure.figsize':(9,7)},style=\"white\")\n",
    "zone_plot = sns.countplot(y=data.LSOA_Name,order = data.LSOA_Name.value_counts().index,palette=\"RdBu\")\n",
    "zone_plot.set(xlabel='Criminal Incidents', ylabel='Borough',xlim=20000)\n",
    "zone_plot.get_xaxis().set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Ranked List of the Most Crime-Ridden Areas\n",
    "crime_ridden_areas = data\n",
    "crime_ridden_areas[\"Sum\"] = 1\n",
    "crime_ridden_areas.groupby([\"LSOA_Name\"]).sum(numeric_only = True).sort_values([\"Sum\"],ascending=False)[\"Sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bar Chart of Crime Incidence by Type\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,7)})\n",
    "zone_plot = sns.countplot(y=data[\"Crime type\"],palette=\"RdBu\")\n",
    "zone_plot.set(xlabel='Criminal Incidents', ylabel='Type of Crime',xlim=5000)\n",
    "zone_plot.get_xaxis().set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Plot of Crime Incidenced Over Time\n",
    "\n",
    "data_temp = data\n",
    "data_temp[\"Counts\"] = 1\n",
    "crime_over_time = data_temp.groupby([\"Month\"]).count()[\"Counts\"]\n",
    "time_plot = sns.lineplot(x=crime_over_time.index,y=crime_over_time,color=\"r\")\n",
    "time_plot.set(xlabel='Time', ylabel='Criminal Incidents')\n",
    "plt.xticks(rotation=30)\n",
    "time_plot.get_yaxis().set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=[\"Month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Data and Plot Geography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "import contextily\n",
    "import category_encoders as ce\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 2.csv\")\n",
    "data[\"Month\"] = data[\"Month\"].astype(\"datetime64[ns]\")\n",
    "data[\"Instances\"] = 1\n",
    "data.insert(0,\"Year\",data.Month.dt.year)\n",
    "data[\"Month\"] = data.Month.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Data from the Top 5 Regions\n",
    "\n",
    "Regions = {\"westminster\" : data.loc[data[\"LSOA_Name\"]==\"Westminster\"],\n",
    "            \"tower hamlets\" : data.loc[data[\"LSOA_Name\"]==\"Tower Hamlets\"],\n",
    "            \"southwark\" : data.loc[data[\"LSOA_Name\"]==\"Southwark\"],\n",
    "            \"newham\" : data.loc[data[\"LSOA_Name\"]==\"Newham\"],\n",
    "            \"lambeth\" : data.loc[data[\"LSOA_Name\"]==\"Lambeth\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Shapefiles for London\n",
    "\n",
    "london_geo_data = gpd.read_file(\"/home/uthlakanyana/Dropbox/Dissertation Code/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\")\n",
    "london_geo_data.to_crs(epsg=4326, inplace=True)\n",
    "london_geo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = data[[\"Year\",\"Month\",\"Longitude\",\"Latitude\",\"LSOA_Name\",\"LSOA11NM\",\"Instances\"]]\n",
    "instances = instances.groupby([\"Year\",\"Month\",\"Longitude\",\"Latitude\",\"LSOA_Name\",\"LSOA11NM\"],as_index=False).sum()\n",
    "instances = instances.merge(london_geo_data[[\"LSOA11NM\",\"geometry\"]],on=\"LSOA11NM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_instances = london_geo_data.merge(instances[[\"LSOA11NM\",\"Instances\"]].groupby([\"LSOA11NM\"],as_index=False).sum())\n",
    "crime_instances[\"geometry\"].to_crs({'init': 'epsg:27700'})\n",
    "crime_instances[\"area\"] = crime_instances.area\n",
    "crime_instances[\"Crime Density\"] = crime_instances[\"Instances\"]/crime_instances[\"area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "crimes = {\"westminster\":pd.DataFrame(),\"newham\":pd.DataFrame(),\"lambeth\":pd.DataFrame(),\"southwark\":pd.DataFrame(),\"tower hamlets\":pd.DataFrame()}\n",
    "\n",
    "for i in crimes.keys():\n",
    "    crimes[i] = crime_instances[crime_instances[\"LAD11NM\"]==i.title()]\n",
    "    num_classes = 4 \n",
    "    num_qtiles = [0, .25, .5, .75, 1.]\n",
    "    qlabels = [\"1st quartile\",\"2nd quartile\",\"3rd quartile\",\"4th quartile\"]\n",
    "    crimes[i].loc[:,'Density_Quartile'] = pd.qcut(crimes[i].loc[:,'Crime Density'], num_qtiles, labels=qlabels)\n",
    "    ax = plt.plot()\n",
    "    ax = crimes[i].plot(column=\"Density_Quartile\",legend=True,cmap=\"coolwarm\",figsize=(7, 7))\n",
    "    ax.set_title(f\"Crime Density Distribution in {i.title()}\")\n",
    "    ax.set_xlabel(\"Latitude\")\n",
    "    ax.set_ylabel(\"Longitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Geographical Dimensions of the Regions in Question\n",
    "\n",
    "for i in Regions.keys():\n",
    "   print(i)\n",
    "   fig, ax = plt.subplots()\n",
    "   ax = london_geo_data[london_geo_data['LAD11NM']== i.title()].boundary.plot(color=\"darkred\",figsize=(7, 7))\n",
    "   ax\n",
    "   ax.set_title(f\"Layout of the {i.title()} Borough\")\n",
    "   ax.set_xlabel(\"Latitude\")\n",
    "   ax.set_ylabel(\"Longitude\")\n",
    "   plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Distribution of Crimes in Each Borough\n",
    "\n",
    "for i in Regions.keys():\n",
    "    ax = plt.plot()\n",
    "    ax = crime_instances[crime_instances['LAD11NM']== i.title()].plot(column=\"Incident_Quartile\",legend=True,cmap=\"coolwarm\",figsize=(7, 7),scheme='quantiles')\n",
    "    ax.set_title(f\"Crime Distribution in {i.title()}\")\n",
    "    ax.set_xlabel(\"Latitude\")\n",
    "    ax.set_ylabel(\"Longitude\")\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Distribution of Household Residents in each Borough\n",
    "\n",
    "HH_Plots = {i:london_geo_data[london_geo_data['LAD11NM']== i.title()].plot(column=\"HHOLDRES\",legend=True,cmap=\"coolwarm\",figsize=(7, 7)).set_title(f\"Household Distribution in {i.title()}\") for i in Regions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Distribution of Population Density in Each Borough\n",
    "\n",
    "PD_Plots = {i:london_geo_data[london_geo_data['LAD11NM']== i.title()].plot(column=\"POPDEN\",legend=True,cmap=\"coolwarm\",figsize=(7, 7)).set_title(f\"Population Density Distribution in the {i.title()}\") for i in Regions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Distribution of Population Density in Each Borough\n",
    "\n",
    "\n",
    "for i in london_geo_data[\"LAD11NM\"].unique():\n",
    "    london_geo_data[london_geo_data[\"LAD11NM\"]==i.title()].plot(column)\n",
    "\n",
    "#PD_Plots = {i:london_geo_data[london_geo_data['LAD11NM']== i.title()].plot(column=\"POPDEN\",legend=True,cmap=\"coolwarm\",figsize=(7, 7)).set_title(f\"Population Density Distribution in the {i.title()} Borough\") for i in Regions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Crime Plotting Function\n",
    "\n",
    "def crime_plotter(region):\n",
    "        # initialize an axis\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        # plot map on axis\n",
    "        london_geo_data[london_geo_data[\"LAD11NM\"] == region.title()].boundary.plot(color=\"darkred\",\n",
    "                                                        ax=ax)\n",
    "\n",
    "        # plot points\n",
    "        Regions[region].plot.scatter(x=\"Longitude\", y=\"Latitude\",ax=ax,s=1,c=\"black\")\n",
    "        # add grid\n",
    "        #ax.grid(alpha=0.5)\n",
    "        ax.set_title(f\"Crime Plot of {region.title()} Borough\")\n",
    "        return plt.show()\n",
    "\n",
    "# Specific Crime plotting Function\n",
    "\n",
    "def specific_crime_plotter(region,crime_type):\n",
    "        # initialize an axis\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        # plot map on axis\n",
    "        london_geo_data[london_geo_data[\"LAD11NM\"] == region.title()].boundary.plot(color=\"darkred\",\n",
    "                                                        ax=ax)\n",
    "\n",
    "        # plot points\n",
    "        Regions[region][Regions[region][\"Crime type\"]==crime_type].plot.scatter(x=\"Longitude\", y=\"Latitude\",ax=ax,s=1,c=\"black\")\n",
    "        # add grid\n",
    "        #ax.grid(alpha=0.5)\n",
    "        ax.set_title(f\"{crime_type.title()} Crime Incidences in {region.title()} Borough\")\n",
    "        return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot General Crime\n",
    "\n",
    "for i in Regions.keys():\n",
    "    crime_plotter(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Types of Crimes\n",
    "\n",
    "data[\"Crime type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Specific Crimes That Take Place in Each Zone\n",
    "\n",
    "for i in Regions.keys():\n",
    "    for v in data[\"Crime type\"].unique():\n",
    "        specific_crime_plotter(i,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Unique LSOAs\n",
    "\n",
    "len(set(data[\"LSOA_Code\"].unique()))\n",
    "data[\"Crime type\"].unique()\n",
    "\n",
    "# Arrange Specific Crimes Into a Dictionary\n",
    "\n",
    "specific_crime = {}\n",
    "groups = pd.concat(Regions.values())\n",
    "\n",
    "for i in groups[\"Crime type\"].unique():\n",
    "    specific_crime[i] = groups[groups[\"Crime type\"]==i]\n",
    "specific_crime[\"Possession of weapons\"]\n",
    "\n",
    "# Generate the Vingtiles For Regions Under Specific Crimes\n",
    "\n",
    "total_specific_crime = {}\n",
    "combined_crime = {}\n",
    "\n",
    "for i in specific_crime.keys():\n",
    "\n",
    "    temp_data = specific_crime[i].copy()[[\"Year\",\"Month\",\"Longitude\",\n",
    "                \"Latitude\",\"LSOA_Code\",\"LSOA_Name\",\n",
    "                \"Working households (thousands)\",\n",
    "                \"Working households (per cent)\",\n",
    "                \"Mixed households (thousands)\",\n",
    "                \"Mixed households (per cent)\",\n",
    "                \"Workless households (thousands)\",\n",
    "                \"Workless households (per cent)\",\n",
    "                \"LSOA11NM\",\"HHOLDRES\",\"COMESTRES\",\n",
    "                \"POPDEN\",\"HHOLDS\",\"AVHHOLDSZ\",\n",
    "                \"Area\",\"Instances\"]]\n",
    "\n",
    "\n",
    "    specific_area_group = temp_data.groupby(by=[\"Year\",\"Month\",\n",
    "            \"LSOA_Code\",\"LSOA_Name\",\n",
    "            \"Working households (thousands)\",\n",
    "            \"Working households (per cent)\",\n",
    "            \"Mixed households (thousands)\",\n",
    "            \"Mixed households (per cent)\",\n",
    "            \"Workless households (thousands)\",\n",
    "            \"Workless households (per cent)\",\n",
    "            \"LSOA11NM\",\"HHOLDRES\",\"COMESTRES\",\n",
    "            \"POPDEN\",\"HHOLDS\",\"AVHHOLDSZ\",\n",
    "            \"Area\"], as_index = False).sum().sort_values(by=[\"Year\",\"Month\",\"LSOA_Name\"],ascending=True)\n",
    "\n",
    "    specific_area_group = specific_area_group.drop(columns=[\"Longitude\",\"Latitude\"])\n",
    "\n",
    "\n",
    "    # Specific Crime Instances Per Zone For Years Up to 2022\n",
    "\n",
    "\n",
    "    #                                                     Select years not 2023         Pick the following columns to form the new DF             Group the new DF by these cols                 Sum the Instances\n",
    "    total_specific_crime[i] = specific_area_group.loc[specific_area_group[\"Year\"]!=2023][[\"LSOA_Code\",\"LSOA11NM\",\"Area\",\"Instances\"]].groupby(by=[\"LSOA_Code\",\"LSOA11NM\",\"Area\"],as_index=False).sum()\n",
    "    total_specific_crime[i][\"Crime Density 2022\"] =  total_specific_crime[i][\"Area\"]/ total_specific_crime[i][\"Instances\"]\n",
    "    total_specific_crime[i]\n",
    "\n",
    "    # Join specific_area_group to total_specific_crime on Basis of LSOA_Code\n",
    "\n",
    "    combined_crime[i] = pd.merge(specific_area_group,total_specific_crime[i][[\"LSOA_Code\",\"Crime Density 2022\"]],on=\"LSOA_Code\")\n",
    "    #specific_crime[i] = specific_area_group\n",
    "\n",
    "total_specific_crime[i]\n",
    "specific_crime[i]\n",
    "specific_area_group\n",
    "total_specific_crime[i]\n",
    "combined_crime[i].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Total Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group The Occurence of Crimes Per LSOA In General\n",
    "\n",
    "temp_data = pd.concat(Regions.values())[[\"Year\",\"Month\",\"Longitude\",\n",
    "                \"Latitude\",\"LSOA_Code\",\"LSOA_Name\",\n",
    "                \"Working households (thousands)\",\n",
    "                \"Working households (per cent)\",\n",
    "                \"Mixed households (thousands)\",\n",
    "                \"Mixed households (per cent)\",\n",
    "                \"Workless households (thousands)\",\n",
    "                \"Workless households (per cent)\",\n",
    "                \"LSOA11NM\",\"HHOLDRES\",\"COMESTRES\",\n",
    "                \"POPDEN\",\"HHOLDS\",\"AVHHOLDSZ\",\n",
    "                \"Area\",\"Instances\"]]\n",
    "\n",
    "\n",
    "\n",
    "per_area_group = temp_data.groupby(by=[\"Year\",\"Month\",\n",
    "                \"LSOA_Code\",\"LSOA_Name\",\n",
    "                \"Working households (thousands)\",\n",
    "                \"Working households (per cent)\",\n",
    "                \"Mixed households (thousands)\",\n",
    "                \"Mixed households (per cent)\",\n",
    "                \"Workless households (thousands)\",\n",
    "                \"Workless households (per cent)\",\n",
    "                \"LSOA11NM\",\"HHOLDRES\",\"COMESTRES\",\n",
    "                \"POPDEN\",\"HHOLDS\",\"AVHHOLDSZ\",\n",
    "                \"Area\"], as_index = False).sum().sort_values(by=[\"Year\",\"Month\",\"LSOA_Name\"],ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "per_area_group = per_area_group.drop(columns=[\"Longitude\",\"Latitude\"])\n",
    "\n",
    "\n",
    "# Annual Crime Instances Per Zone For Years not 2023\n",
    "\n",
    "total_crime = per_area_group.loc[per_area_group[\"Year\"]!=2023][[\"LSOA_Code\",\"LSOA11NM\",\"Area\",\"Instances\"]].groupby(by=[\"LSOA_Code\",\"LSOA11NM\",\"Area\"],as_index=False).sum()\n",
    "total_crime[\"Crime Density 2022\"] = total_crime[\"Area\"]/total_crime[\"Instances\"]\n",
    "total_crime\n",
    "\n",
    "# Join per_area_group to annual_total_crime on Basis of LSOA_Code\n",
    "\n",
    "per_area_group = pd.merge(per_area_group,total_crime[[\"LSOA_Code\",\"Crime Density 2022\"]],on=\"LSOA_Code\")\n",
    "per_area_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Year-Month Tuples to Fill In Any Gaps Due to Months Not Having Any Crime\n",
    "\n",
    "tuples = [(2020,7),(2020,8),(2020,9),(2020,10),(2020,11),(2020,12),\n",
    "          (2021,1),(2021,2),(2021,3),(2021,4),(2021,5),(2021,6),(2021,7),(2021,8),(2021,9),(2021,10),(2021,11),(2021,12),\n",
    "          (2022,1),(2022,2),(2022,3),(2022,4),(2022,5),(2022,6),(2022,7),(2022,8),(2022,9),(2022,10),(2022,11),(2022,12),\n",
    "          (2023,1),(2023,2),(2023,3),(2023,4),(2023,5),(2023,6)]\n",
    "\n",
    "#locations = per_area_group[\"LSOA11NM\"].unique()\n",
    "\n",
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Fill Missing Months With Crime Values\n",
    "\n",
    "def new_filler(dataframe,locations,tuples=tuples):\n",
    "    \n",
    "    dataframe = dataframe\n",
    "    locations = locations\n",
    "\n",
    "    for l in locations:\n",
    "\n",
    "        print(l)\n",
    "\n",
    "        if len(dataframe[dataframe['LSOA11NM']==l]) != 36:\n",
    "\n",
    "            row = dataframe[dataframe['LSOA11NM']==l].iloc[0]\n",
    "            print()\n",
    "            print(l,f\" has {len(dataframe[dataframe['LSOA11NM']==l])} entries\")\n",
    "\n",
    "            for t in range(len(tuples)):\n",
    "                #print(tuples[t])\n",
    "\n",
    "                if len(dataframe[dataframe[\"LSOA11NM\"]==l][dataframe[\"Year\"]==tuples[t][0]][dataframe[\"Month\"]==tuples[t][1]]) == 0:\n",
    "                    print(tuples[t])\n",
    "                    #row = dataframe[dataframe['LSOA11NM'==l]][dataframe[\"Year\"]==tuples[t][0]][dataframe[\"Month\"]==tuples[t][1]].iloc[0]\n",
    "                    dataframe = dataframe.append(row, ignore_index=True)\n",
    "                    #print(row)\n",
    "                    dataframe.at[dataframe.index[-1],\"Instances\"] = 0\n",
    "                    dataframe.at[dataframe.index[-1],\"Year\"] = tuples[t][0]\n",
    "                    dataframe.at[dataframe.index[-1],\"Month\"] = tuples[t][1]\n",
    "                    print(dataframe.columns)\n",
    "    return dataframe\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute General Crime With Missing Months\n",
    "\n",
    "per_area_group = new_filler(per_area_group,locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Specific Crime with Missing Months\n",
    "combined_crime2 = dict.fromkeys(combined_crime)\n",
    "\n",
    "\n",
    "for i in combined_crime.keys():\n",
    "    combined_crime2[i] = new_filler(combined_crime[i],combined_crime[i][\"LSOA11NM\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Length of Imputed Values for Singular Crimes\n",
    "\n",
    "for i in combined_crime2.keys():\n",
    "    print(i,len(combined_crime2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation/Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check That the Length of the Dataset is Similar to What is Expected of 4,828 LSOAs over 19 Months (Some LSOAs Might Not Have Crimes Over All 17 Months) \n",
    "# Expanded Dataset Means Original Time and Regions have Changed. The Time Period is 36 Months Now\n",
    "\n",
    "(len(set(per_area_group[\"LSOA_Code\"].unique())) * 36) == len(per_area_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Ceneral Crime Into Boroughs\n",
    "\n",
    "new_regions = {\"westminster\" : per_area_group.loc[per_area_group[\"LSOA_Name\"]==\"Westminster\"],\n",
    "            \"tower hamlets\" : per_area_group.loc[per_area_group[\"LSOA_Name\"]==\"Tower Hamlets\"],\n",
    "            \"southwark\" : per_area_group.loc[per_area_group[\"LSOA_Name\"]==\"Southwark\"],\n",
    "            \"newham\" : per_area_group.loc[per_area_group[\"LSOA_Name\"]==\"Newham\"],\n",
    "            \"lambeth\" : per_area_group.loc[per_area_group[\"LSOA_Name\"]==\"Lambeth\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the Same As Above But For Specific Crimes\n",
    "\n",
    "crime_type = {}\n",
    "\n",
    "for i in combined_crime2.keys():\n",
    "    crime_type[i] = {\"westminster\" : combined_crime2[i][combined_crime2[i][\"LSOA_Name\"]==\"Westminster\"],\n",
    "            \"tower hamlets\" : combined_crime2[i].loc[combined_crime2[i][\"LSOA_Name\"]==\"Tower Hamlets\"],\n",
    "            \"southwark\" : combined_crime2[i].loc[combined_crime2[i][\"LSOA_Name\"]==\"Southwark\"],\n",
    "            \"newham\" : combined_crime2[i].loc[combined_crime2[i][\"LSOA_Name\"]==\"Newham\"],\n",
    "            \"lambeth\" : combined_crime2[i].loc[combined_crime2[i][\"LSOA_Name\"]==\"Lambeth\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Stuff\n",
    "\n",
    "crime_type[i][\"southwark\"].sort_values(by=[\"Year\",\"Month\"]).head()\n",
    "new_regions[\"southwark\"][new_regions[\"southwark\"][\"Year\"]==2022]\n",
    "per_area_group.loc[per_area_group[\"Year\"]==2022][[\"LSOA_Code\",\"LSOA11NM\",\"Area\",\"Instances\"]].groupby(by=[\"LSOA_Code\",\"LSOA11NM\",\"Area\"],as_index=False).sum()\n",
    "len(new_regions[\"southwark\"][\"LSOA_Code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the Number of LSOA Occurences Across Months\n",
    "\n",
    "new_regions[\"southwark\"].groupby(by=[\"LSOA_Code\"]).count()\n",
    "\n",
    "# List Number of Occurrences\n",
    "\n",
    "print(new_regions[\"lambeth\"].groupby(by=[\"LSOA_Code\"]).count()[\"Year\"].unique())\n",
    "\n",
    "# It Can be Seen that Some LSOA's Don't Report Crimes For All 17/36 Months, But None Has Reported for More Than 19/36 Months\n",
    "# Corrected with Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Encoder for the Various Datasets\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols='LSOA_Code')\n",
    "# Identify Total Starting Features\n",
    "\n",
    "og_columns = set(new_regions[\"southwark\"].columns)\n",
    "og_columns\n",
    "# Transformer Function\n",
    "\n",
    "def btransformer(dataframe,old_cols=og_columns):\n",
    "    \n",
    "    # Call on Original Columns\n",
    "\n",
    "    old_cols = og_columns\n",
    "\n",
    "    # Establish Pre-Transformation Length\n",
    "\n",
    "    pre_length = len(dataframe[\"LSOA_Code\"].unique())\n",
    "\n",
    "    # Transform Data and Reassign to Dictionary\n",
    "\n",
    "    dataframe = encoder.fit_transform(dataframe)\n",
    "\n",
    "    # Identify New Columns After Transformation\n",
    "\n",
    "    new_cols = set(dataframe.columns)\n",
    "\n",
    "    # Identify Unique New Columns\n",
    "\n",
    "    diffs = list(new_cols - old_cols)\n",
    "\n",
    "    # Calculate Post-Transformation Length\n",
    "    post_length = len(dataframe.groupby(by=diffs).count())\n",
    "\n",
    "    # If Post-Transformation Length Is Less than Pre-Transformation Length, then Collisions Have occured\n",
    "    #print(pre_length,post_length)\n",
    "\n",
    "    print(i,\":\",f\"Collisions = {post_length-pre_length}\")\n",
    "\n",
    "    return dataframe\n",
    "# Transform and Check for Potential Collisions\n",
    "\n",
    "for i in new_regions.keys():\n",
    "    new_regions[i] = btransformer(new_regions[i])\n",
    "i\n",
    "# Same as Above but For Specific Crimes\n",
    "\n",
    "for i in crime_type.keys():\n",
    "    print(i)\n",
    "    for q in crime_type[i].keys():\n",
    "        print(q)\n",
    "        crime_type[i][q] = btransformer(crime_type[i][q])\n",
    "crime_type[\"Other theft\"][\"southwark\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotspot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the Vingtile Distributions For Specific Crimes\n",
    "\n",
    "for i in crime_type.keys():\n",
    "    for q in crime_type[i].keys():\n",
    "        crime_type[i][q].loc[:,\"Vingtile Rank\"] = pd.qcut(crime_type[i][q][\"Crime Density 2022\"],20,labels=False,duplicates=\"drop\")\n",
    "        crime_type[i][q] = crime_type[i][q].sort_values(by=[\"Year\",\"Month\",\"Vingtile Rank\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type[\"Anti-social behaviour\"][\"westminster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the The Sub Regions That Are Within the Top 5% In Terms of Crime Density For Each Region\n",
    "\n",
    "specific_region_hotspots = dict.fromkeys(crime_type)\n",
    "\n",
    "for k in specific_region_hotspots.keys():\n",
    "    specific_region_hotspots[k] = {\"southwark\":0,\"tower hamlets\":0,\"westminster\":0,\"lambeth\":0,\"newham\":0}\n",
    "\n",
    "for i in crime_type.keys():\n",
    "    for q in crime_type[i].keys():\n",
    "        specific_region_hotspots[i][q] = list(crime_type[i][q][crime_type[i][q][\"Vingtile Rank\"]==19][\"LSOA11NM\"].unique())\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type[\"Theft from the person\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the Vingtile Distributions For General Crimes\n",
    "\n",
    "for i in new_regions.keys():\n",
    "    new_regions[i].loc[:,\"Vingtile Rank\"] = pd.qcut(new_regions[i][\"Crime Density 2022\"],20,labels=False)\n",
    "    new_regions[i] = new_regions[i].sort_values(by=[\"Year\",\"Month\",\"Vingtile Rank\"],ascending=False)\n",
    "\n",
    "new_regions[i][[\"Year\",\"Month\",\"LSOA11NM\",\"Vingtile Rank\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the The Sub Regions That Are Within the Top 5% In Terms of Crime Density For Each Region\n",
    "\n",
    "region_hotspots = {}\n",
    "\n",
    "for i in new_regions.keys():\n",
    "    region_hotspots[i] = list(new_regions[i][new_regions[i][\"Vingtile Rank\"]==19][\"LSOA11NM\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_regions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_region_hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Export and Tag Specific Crime Datasets\n",
    "\n",
    "def specific_exporter(dataframe,title):\n",
    "    dataframe = dataframe[dataframe[\"Vingtile Rank\"]==19]\n",
    "    dataframe = dataframe[[\"Year\",\"Month\",\"Working households (thousands)\",\n",
    "                     \"Working households (per cent)\",\"Mixed households (thousands)\",\n",
    "                     \"Mixed households (per cent)\",\"Workless households (thousands)\",\n",
    "                     \"Workless households (per cent)\",\"LSOA11NM\",\"COMESTRES\",\"POPDEN\",\n",
    "                     \"HHOLDS\",\"AVHHOLDSZ\",\"Area\",\"Instances\",\"Crime Density 2022\"]]\n",
    "    dataframe[\"Crime Type\"] = title\n",
    "    dataframe = dataframe.sort_values(by=[\"Year\",\"Month\"])\n",
    "\n",
    "    return dataframe.to_csv(f\"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset/{title}/{i}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Exporting Function for Specific Datasets\n",
    "\n",
    "for i in crime_type.keys():\n",
    "        directory = i\n",
    "        parent_dir = \"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset/\"\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        os.mkdir(path)\n",
    "\n",
    "        crime_type[i] = specific_exporter(pd.concat(crime_type[i].values()),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_crime.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Dataset Prep\n",
    "\n",
    "full_set = pd.concat(new_regions.values())\n",
    "full_set = full_set[full_set[\"Vingtile Rank\"]==19]\n",
    "full_set = full_set[[\"Year\",\"Month\",\"Working households (thousands)\",\n",
    "                     \"Working households (per cent)\",\"Mixed households (thousands)\",\n",
    "                     \"Mixed households (per cent)\",\"Workless households (thousands)\",\n",
    "                     \"Workless households (per cent)\",\"LSOA11NM\",\"COMESTRES\",\"POPDEN\",\n",
    "                     \"HHOLDS\",\"AVHHOLDSZ\",\"Area\",\"Instances\",\"Crime Density 2022\"]]\n",
    "full_set = full_set.sort_values(by=[\"Year\",\"Month\"])\n",
    "full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Full Dataset\n",
    "\n",
    "full_set.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Modules\n",
    "\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "import regex as re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter Data\n",
    "\n",
    "twitter_data = {\"southwark\":pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Tweet Data 2/MPS Southwark.csv\",usecols=[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"view_count\"]),\n",
    "                \"lambeth\":pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Tweet Data 2/MPS Lambeth.csv\",usecols=[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"view_count\"]),\n",
    "                \"newham\":pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Tweet Data 2/MPS Newham.csv\",usecols=[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"view_count\"]),\n",
    "                \"tower hamlets\":pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Tweet Data 2/MPS Tower Hamlets.csv\",usecols=[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"view_count\"]),\n",
    "                \"westminster\":pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Tweet Data 2/MPS Westminster.csv\",usecols=[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"view_count\"])\n",
    "                }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"newham\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set Pandas Column Width to Larger\n",
    "\n",
    "pd.options.display.max_colwidth = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate the Sentiment\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    # Run VADER on the text\n",
    "    scores = sentimentAnalyser.polarity_scores(text)\n",
    "    # Extract the compound score\n",
    "    compound_score = scores['compound']\n",
    "    # Return compound score\n",
    "    return compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Apply calculate_sentiment Function to Every Dataframe\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    twitter_data[i][\"text\"] = twitter_data[i][\"text\"].str.replace(\"\\n\",\" \")\n",
    "    twitter_data[i]['sentiment_score'] = twitter_data[i]['text'].apply(calculate_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"newham\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Punctuation Characters to punct_cars\n",
    "\n",
    "punct_chars = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip Punctuation\n",
    "\n",
    "#for i in twitter_data.keys():\n",
    "#    twitter_data[i]['text'] = twitter_data[i]['text'].apply(lambda x: ' '.join(char for char in x if char not in punct_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Correct All  Entries and Their Dtypes\n",
    "\n",
    "def correcter(dataframe):\n",
    "\n",
    "    # Change \"created_at\" column to datetime\n",
    "    dataframe[\"created_at\"] = pd.to_datetime(dataframe[\"created_at\"],errors=\"coerce\")\n",
    "\n",
    "    # Rearrange dataframe columns\n",
    "    dataframe = dataframe[[\"created_at\",\"text\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\",\"sentiment_score\"]]\n",
    "    \n",
    "    # Set Year and Month\n",
    "    dataframe.insert(0,\"Year\",dataframe.created_at.dt.year)\n",
    "    dataframe.insert(1,\"Month\",dataframe.created_at.dt.month)\n",
    "\n",
    "    # Drop Created_At Column\n",
    "\n",
    "    dataframe = dataframe.drop(columns=[\"created_at\"],axis=1)\n",
    "\n",
    "    # Clean individual tweets\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe.loc[i,\"text\"] = p.clean(dataframe.iloc[i][\"text\"])\n",
    "\n",
    "    # Drop NA values (relevant for Westminster alone)\n",
    "\n",
    "    dataframe = dataframe.dropna(subset=[\"text\",\"Year\",\"Month\"])\n",
    "\n",
    "\n",
    "    #dataframe = dataframe.dropna(subset=[\"created_at\",\"text\"])\n",
    "\n",
    "    \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Correcting Function to All Dataframes\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    print(i,\"beginning -\",len(twitter_data[i]))\n",
    "    twitter_data[i] = correcter(twitter_data[i])\n",
    "    print(i, \"end -\",len(twitter_data[i]))\n",
    "\n",
    "# Note that Westminster Loses Some Entries Due to Errors in the Scraping Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"newham\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Lemmatize and Tokenize\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    " return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize All Tweets in Dataframes\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    twitter_data[i][\"text\"] = twitter_data[i][\"text\"].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"newham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words and Punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    twitter_data[i][\"text\"] =  twitter_data[i][\"text\"].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    twitter_data[i][\"text\"] = twitter_data[i][\"text\"].apply(lambda x: [i for i in x if i not in punct_chars])\n",
    "    twitter_data[i] = twitter_data[i].sort_values(by=[\"Year\",\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"newham\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Text and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Average of Sentiment Scores on a Monthly Basis\n",
    "\n",
    "monthly_sentiments = dict.fromkeys(twitter_data)\n",
    "\n",
    "for i in monthly_sentiments.keys():\n",
    "    monthly_sentiments[i] = twitter_data[i][[\"Year\",\"Month\",\"sentiment_score\"]].groupby([\"Year\",\"Month\"],as_index=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show That Monthly Sentiments Are Grouped Properly\n",
    "\n",
    "monthly_sentiments[\"newham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Group Twitter Non-text Features by Year and Month \n",
    "\n",
    "def compiler(dataframe):\n",
    "    new_dataframe = dataframe[[\"Year\",\"Month\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\"]].groupby([\"Year\",\"Month\"],as_index=False).sum()\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Dtypes\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    twitter_data[i].loc[:,\"Year\"] = twitter_data[i].loc[:,\"Year\"].astype(\"int\")\n",
    "    twitter_data[i].loc[:,\"Month\"] = twitter_data[i].loc[:,\"Month\"].astype(\"int\")\n",
    "    twitter_data[i].loc[:,\"favorite_count\"] = twitter_data[i].loc[:,\"favorite_count\"].astype(\"int\")\n",
    "    twitter_data[i].loc[:,\"bookmark_count\"] = twitter_data[i].loc[:,\"bookmark_count\"].astype(\"int\")\n",
    "    twitter_data[i].loc[:,\"retweet_count\"] = twitter_data[i].loc[:,\"retweet_count\"].astype(\"int\")\n",
    "    twitter_data[i].loc[:,\"reply_count\"] = twitter_data[i].loc[:,\"reply_count\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that Reassingment of Dtypes Worked\n",
    "\n",
    "twitter_data[i].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary that Stores Grouped Twitter Non-Text Data\n",
    "\n",
    "compiled_tweet_features = dict.fromkeys(twitter_data)\n",
    "\n",
    "for i in compiled_tweet_features:\n",
    "    compiled_tweet_features[i] = compiler(twitter_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tweet_features[\"newham\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Text and Non-Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tack on Sentiment Score to the Compiled tweet Properties\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    compiled_tweet_features[i].loc[:,\"average_sentiment\"] = monthly_sentiments[i].loc[:,\"sentiment_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tweet_features[\"newham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary to Store Grouped textual Data\n",
    "\n",
    "textual_collations = dict.fromkeys(twitter_data)\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    textual_collations[i] = twitter_data[i][[\"Year\",\"Month\",\"text\"]].groupby([\"Year\",\"Month\"],as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Remove Punctuations from the Lists \n",
    "\n",
    "def punctuation_remover(text):\n",
    "    new_list = [i for i in text if i not in string.punctuation]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations From the Listified Text Data for Each Sub-Region\n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    textual_collations[i].loc[:,\"text\"] = textual_collations[i].loc[:,\"text\"].apply(lambda x: punctuation_remover(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_collations[\"newham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tweet_features[\"newham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Grouped textual and Non-Textual Data\n",
    "\n",
    "for i in compiled_tweet_features:\n",
    "    print(i)\n",
    "    compiled_tweet_features[i] = compiled_tweet_features[i].merge(textual_collations[i],left_on=[\"Year\",\"Month\"],right_on=[\"Year\",\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tweet_features[\"newham\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Social Media Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_twitter_data = twitter_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in graph_twitter_data.keys():\n",
    "    graph_twitter_data[i][\"Date\"] = graph_twitter_data[i][\"Year\"].astype(str) + \"-\" + graph_twitter_data[i][\"Month\"].astype(str)\n",
    "    graph_twitter_data[i][\"Date\"] = graph_twitter_data[i][\"Date\"].astype(\"datetime64\")\n",
    "    graph_twitter_data[i] = graph_twitter_data[i][[\"Date\",\"sentiment_score\",\"bookmark_count\",\"favorite_count\",\"retweet_count\",\"reply_count\"]].groupby([\"Date\"],as_index=False).agg({\"sentiment_score\":\"mean\",\"bookmark_count\":\"sum\",\"favorite_count\":\"sum\",\"retweet_count\":\"sum\",\"reply_count\":\"sum\"})\n",
    "    graph_twitter_data[i] = graph_twitter_data[i].sort_values(by=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_twitter_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Social Media Behaviour for Each MPS\n",
    "\n",
    "for i in graph_twitter_data.keys():\n",
    "    ax = graph_twitter_data[i].plot(x=\"Date\",y=\"sentiment_score\",legend=False,color=\"r\")\n",
    "    ax2 = ax.twinx()\n",
    "    graph_twitter_data[i].plot(x=\"Date\",y=\"bookmark_count\",ax=ax2,legend=False)\n",
    "    ax.figure.legend()\n",
    "    plt.title(f\"Sentiment/Bookmark Patterns for MPS {i.title()}\",x=0.32)\n",
    "    ax.set_ylabel('Sentiment Score')\n",
    "    ax2.set_ylabel(\"Bookmark Count\")\n",
    "\n",
    "\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Social Media Behaviour for Each MPS (Sentiment)\n",
    "\n",
    "for i in graph_twitter_data.keys():\n",
    "    ax = graph_twitter_data[i].plot(x=\"Date\",y=\"sentiment_score\",legend=False,color=\"r\")\n",
    "    plt.title(f\"VADER Sentiment Over Time for MPS {i.title()}\")\n",
    "    ax.set_ylabel('Sentiment Score')\n",
    "\n",
    "\n",
    "    #plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_twitter_data[i].corr().style.background_gradient(cmap='coolwarm').set_precision(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Columns for Name of Region \n",
    "\n",
    "for i in twitter_data.keys():\n",
    "    compiled_tweet_features[i].loc[:,\"LSOA_Name\"] = i.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc = pd.concat(compiled_tweet_features.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Grouped Social Media Data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Comprehensive Data and Textual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Modules\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load General Crime Data\n",
    "\n",
    "comp_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 3.csv\").sort_values(by=[\"Year\",\"Month\"])\n",
    "tweet_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Grouped Social Media Data.csv\").sort_values(by=[\"Year\",\"Month\"])\n",
    "specific_lsoa_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Specific LSOA Data.csv\")\n",
    "\n",
    "# Directory Navigation\n",
    "d = os.getcwd()\n",
    "print(f'Current: {d}')\n",
    "file_location = \"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset\"\n",
    "os.chdir(file_location)\n",
    "\n",
    "# Obtain Directory Items\n",
    "locations = [file_location+\"/\"+str(f) for f in os.listdir()]\n",
    "    \n",
    "# Obtain the Files Locations in Each Directory\n",
    "dataframes = []\n",
    "for l in locations:\n",
    "    os.chdir(l)\n",
    "    dataframes.extend([l+\"/\"+i for i in os.listdir(l)])\n",
    "\n",
    "# Load Specific Crime Data\n",
    "\n",
    "specific_crimes = {}\n",
    "\n",
    "for d in dataframes:\n",
    "    specific_crimes[d.rsplit(\"/\")[-1].rsplit(\".\")[0]] = pd.read_csv(d).sort_values(by=[\"Year\",\"Month\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New Column For comp_data For Prior Instances (This Will Help Generate Crime Motions later On)\n",
    "\n",
    "comp_data[\"Prior Instances\"] = np.nan\n",
    "\n",
    "for i in specific_crimes.keys():\n",
    "    specific_crimes[i][\"Prior Instances\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_function(dataframe):\n",
    "\n",
    "    # Create Unique List of LSOA Names for General Crimes\n",
    "\n",
    "    UniqueNames = dataframe.LSOA11NM.unique()\n",
    "\n",
    "    # Store LSOA Dictionaries in Dataframe\n",
    "    DataFrameDict = {elem : pd.DataFrame() for elem in UniqueNames}\n",
    "\n",
    "    for key in DataFrameDict.keys():\n",
    "        DataFrameDict[key] = dataframe[:][dataframe.LSOA11NM == key]\n",
    "\n",
    "    # Lag the Split Dataframes\n",
    "\n",
    "    for key in DataFrameDict.keys():\n",
    "        DataFrameDict[key][\"Prior Instances\"] = DataFrameDict[key][\"Instances\"].shift()\n",
    "        \n",
    "    return DataFrameDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Unique List of LSOA Names for General Crimes\n",
    "\n",
    "#UniqueNames = comp_data.LSOA11NM.unique()\n",
    "\n",
    "# Store LSOA Dictionaries in Dataframe\n",
    "#DataFrameDict = {elem : pd.DataFrame() for elem in UniqueNames}\n",
    "\n",
    "#for key in DataFrameDict.keys():\n",
    "#    DataFrameDict[key] = comp_data[:][comp_data.LSOA11NM == key]\n",
    "\n",
    "# Lag the Split Dataframes\n",
    "\n",
    "#for key in DataFrameDict.keys():\n",
    "#    DataFrameDict[key][\"Prior Instances\"] = DataFrameDict[key][\"Instances\"].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp_data[\"LSOA11NM\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Lag Function on General Crimes\n",
    "\n",
    "comp_data = pd.concat(lag_function(comp_data).values())\n",
    "comp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data[comp_data[\"Crime Density 2022\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call lag Function on Specific Crimes\n",
    "\n",
    "specific_crimes2 = dict.fromkeys(specific_crimes)\n",
    "\n",
    "for i in specific_crimes2:\n",
    "    specific_crimes2[i] = pd.concat(lag_function(specific_crimes[i]).values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_crimes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert Raw Differences To General Directions\n",
    "\n",
    "def movement_mapping(datum):\n",
    "\n",
    "    # Increase Vs Decrease or Stay the Same\n",
    "    if datum == 0:\n",
    "        datum = 0\n",
    "    elif datum >= 0:\n",
    "        datum = 1\n",
    "    elif datum <= 0:\n",
    "        datum = 0\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'Differences' Columns for General Crime\n",
    "\n",
    "comp_data = comp_data.sort_values(by=[\"Year\",\"Month\"]).reset_index(drop=True)\n",
    "comp_data[\"Differences\"] = comp_data[\"Instances\"] - comp_data[\"Prior Instances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange General Crime dataframe\n",
    "\n",
    "comp_data = comp_data.copy()[[\"Year\",\"Month\",\"LSOA11NM\",\"Working households (thousands)\",\n",
    "                                \"Working households (per cent)\",\"Mixed households (thousands)\",\n",
    "                                \"Mixed households (per cent)\",\"Workless households (thousands)\",\n",
    "                                \"Workless households (per cent)\",\"COMESTRES\",\"POPDEN\",\n",
    "                                \"HHOLDS\",\"AVHHOLDSZ\",\"Area\",\"Crime Density 2022\",\"Differences\"]]\n",
    "comp_data\n",
    "\n",
    "comp_data.columns = comp_data.columns.astype(str)\n",
    "comp_data.loc[:,'Differences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Proportion of Data Where Differences Are 0\n",
    "\n",
    "(len(comp_data[comp_data[\"Differences\"]==0])/len(comp_data))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace The Original Difference Column With Directions\n",
    "\n",
    "comp_data.loc[:,\"Differences\"] = comp_data.loc[:,\"Differences\"].apply(lambda x:movement_mapping(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resplit Data According to Borough\n",
    "\n",
    "resplit = {\n",
    "    \"westminster\":comp_data[comp_data[\"LSOA11NM\"].str.contains(\"Westminster\")],\n",
    "    \"southwark\":comp_data[comp_data[\"LSOA11NM\"].str.contains(\"Southwark\")],\n",
    "    \"tower hamlets\":comp_data[comp_data[\"LSOA11NM\"].str.contains(\"Tower Hamlets\")],\n",
    "    \"lambeth\":comp_data[comp_data[\"LSOA11NM\"].str.contains(\"Lambeth\")],\n",
    "    \"newham\":comp_data[comp_data[\"LSOA11NM\"].str.contains(\"Newham\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'Differences' Columns for Specific Crime\n",
    "\n",
    "for i in specific_crimes2:\n",
    "    specific_crimes2[i] = specific_crimes2[i].sort_values(by=[\"Year\",\"Month\"]).reset_index(drop=True)\n",
    "    specific_crimes2[i][\"Differences\"] = specific_crimes2[i][\"Instances\"] - specific_crimes2[i][\"Prior Instances\"]\n",
    "\n",
    "specific_crimes2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Specific Crime Dataframes\n",
    "\n",
    "for i in specific_crimes2:\n",
    "    specific_crimes2[i] = specific_crimes2[i].copy()[[\"Year\",\"Month\",\"LSOA11NM\",\"Working households (thousands)\",\n",
    "                                                      \"Working households (per cent)\",\"Mixed households (thousands)\",\n",
    "                                                      \"Mixed households (per cent)\",\"Workless households (thousands)\",\n",
    "                                                      \"Workless households (per cent)\",\"COMESTRES\",\"POPDEN\",\n",
    "                                                      \"HHOLDS\",\"AVHHOLDSZ\",\"Area\",\"Crime Density 2022\",\"Crime Type\",\"Differences\"]]\n",
    "\n",
    "\n",
    "    specific_crimes2[i].columns = specific_crimes2[i].columns.astype(str)\n",
    "    specific_crimes2[i].loc[:,\"Differences\"] = specific_crimes2[i].loc[:,\"Differences\"].apply(lambda x:movement_mapping(x))\n",
    "    specific_crimes2[i] = specific_crimes2[i].sort_values([\"Year\",\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_crimes3 = dict.fromkeys(specific_crimes2)\n",
    "\n",
    "for i in specific_crimes3.keys():\n",
    "    specific_crimes3[i] ={\n",
    "                          \"westminster\":specific_crimes2[i][specific_crimes2[i][\"LSOA11NM\"].str.contains(\"Westminster\")],\n",
    "                          \"southwark\":specific_crimes2[i][specific_crimes2[i][\"LSOA11NM\"].str.contains(\"Southwark\")],\n",
    "                          \"tower hamlets\":specific_crimes2[i][specific_crimes2[i][\"LSOA11NM\"].str.contains(\"Tower Hamlets\")],\n",
    "                          \"lambeth\":specific_crimes2[i][specific_crimes2[i][\"LSOA11NM\"].str.contains(\"Lambeth\")],\n",
    "                          \"newham\":specific_crimes2[i][specific_crimes2[i][\"LSOA11NM\"].str.contains(\"Newham\")]\n",
    "                          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merger Function\n",
    "\n",
    "def merger(dataframe,iterator,tweet_data=tweet_data):\n",
    "    dataframe = pd.merge(dataframe,tweet_data.loc[tweet_data[\"LSOA_Name\"]==iterator.title()],how=\"outer\")\n",
    "    dataframe = dataframe[dataframe[\"Working households (thousands)\"].isna()==False]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge General Crime Data with Textual Information\n",
    "\n",
    "for i in resplit.keys():\n",
    "    resplit[i] = merger(resplit[i],i)\n",
    "    #resplit[i] = pd.merge(resplit[i],tweet_data.loc[tweet_data[\"LSOA_Name\"]==i.title()],how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine General Crime Data\n",
    "\n",
    "new_comp_data = pd.concat(resplit.values(),axis=0)\n",
    "new_comp_data = new_comp_data[new_comp_data[\"Area\"].isna()!=True]\n",
    "new_comp_data = new_comp_data[new_comp_data[\"Differences\"].isna()!=True]\n",
    "new_comp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Missing Values in the Combined DataSet After Adding Columns\n",
    "\n",
    "for i in new_comp_data.columns:\n",
    "    print(i,new_comp_data[i].isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Specific Crime Data with Textual Information\n",
    "\n",
    "for crime in specific_crimes3.keys():\n",
    "    for region in specific_crimes3[crime]:\n",
    "        specific_crimes3[crime][region] = merger(specific_crimes3[crime][region],region)\n",
    "    #resplit[i] = pd.merge(resplit[i],tweet_data.loc[tweet_data[\"LSOA_Name\"]==i.title()],how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm That There Is Variance in Crime/Location Combinations\n",
    "\n",
    "for i in specific_crimes3.keys():\n",
    "    for p in specific_crimes3[i].keys():\n",
    "        print(i,p,len(specific_crimes3[i][p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine Specific Crime Data\n",
    "specific_crimes4 = dict.fromkeys(specific_crimes3)\n",
    "\n",
    "for i in specific_crimes3.keys():\n",
    "    specific_crimes4[i] = pd.concat(specific_crimes3[i].values(),axis=0)\n",
    "    specific_crimes4[i] = specific_crimes4[i][specific_crimes4[i][\"Area\"].isna()!=True]\n",
    "    specific_crimes4[i] = specific_crimes4[i][specific_crimes4[i][\"Differences\"].isna()!=True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.merge(new_comp_data,specific_lsoa_data,on=\"LSOA11NM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Export and Tag Specific Crime Datasets\n",
    "\n",
    "def specific_exporter(dataframe,title):\n",
    "    dataframe[\"Crime Type\"] = title\n",
    "    dataframe = dataframe.sort_values(by=[\"Year\",\"Month\"])\n",
    "\n",
    "    return dataframe.to_csv(f\"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset/{title}/{i}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in specific_crimes4.keys():\n",
    "    specific_crimes4[i] = pd.merge(specific_crimes4[i],specific_lsoa_data,on=\"LSOA11NM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in specific_crimes4.keys():\n",
    "    specific_crimes4.copy()[i] = specific_exporter(specific_crimes4[i],i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import little_mallet_wrapper\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "pd.options.display.max_colwidth = 100\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 4.csv\")\n",
    "\n",
    "# Function to Convert the Stringified Lists Into Proper Lists\n",
    "\n",
    "def fix_list(text):\n",
    "    try:\n",
    "        #print(text)\n",
    "        text = eval(text)\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    except:\n",
    "        print(text)\n",
    "\n",
    "\n",
    "\n",
    "data.loc[:,\"text\"] = data.loc[:,\"text\"].apply(lambda x: fix_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Training and Test Sets\n",
    "\n",
    "training = data[data[\"Year\"]==2022]\n",
    "test = data[data[\"Year\"]==2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TFIDF Vectorization\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=list(stop_words),max_features=1000)\n",
    "vect_text=vect.fit_transform(training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LDA Model\n",
    "\n",
    "lda_model=LatentDirichletAllocation(n_components=30,learning_method='online',random_state=1,max_iter=1)\n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Topics\n",
    "\n",
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Topic Features\n",
    "\n",
    "vocab = vect.get_feature_names_out()\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "     print(\"Topic \"+str(i)+\": \")\n",
    "     for t in sorted_words:\n",
    "            print(t[0],end=\" \")\n",
    "            print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALLET Approach to TM\n",
    "\n",
    "little_mallet_wrapper.print_dataset_stats(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Number of Topics\n",
    "num_topics = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to Your Desired Output Directory\n",
    "output_directory_path = '/home/uthlakanyana/Dropbox/Dissertation Code/Police Tweets Topics'\n",
    "\n",
    "path_to_mallet = \"/usr/bin/mallet\"\n",
    "\n",
    "#No need to change anything below here\n",
    "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
    "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
    "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little_mallet_wrapper.quick_train_topic_model(path_to_mallet,output_directory_path,num_topics,training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Weather and To Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Modules\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load Data\n",
    "\n",
    "weather = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Heathrow Met Data.csv\",sep=\",\")\n",
    "weather.columns = [\"Year\",\"Month\",\"Max_Temp\",\"Min_Temp\",\"Frost Days\",\"Rain\",\"Sun\"]\n",
    "full_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 4.csv\")\n",
    "\n",
    "# Directory Navigation\n",
    "d = os.getcwd()\n",
    "print(f'Current: {d}')\n",
    "file_location = \"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset\"\n",
    "os.chdir(file_location)\n",
    "\n",
    "# Obtain Directory Items\n",
    "locations = [file_location+\"/\"+str(f) for f in os.listdir()]\n",
    "    \n",
    "# Obtain the Files Locations in Each Directory\n",
    "dataframes = []\n",
    "for l in locations:\n",
    "    os.chdir(l)\n",
    "    dataframes.extend([l+\"/\"+i for i in os.listdir(l)])\n",
    "\n",
    "# Load Specific Crime Data\n",
    "\n",
    "specific_crimes = {}\n",
    "\n",
    "for d in dataframes:\n",
    "    specific_crimes[d.rsplit(\"/\")[-1].rsplit(\".\")[0]] = pd.read_csv(d).sort_values(by=[\"Year\",\"Month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Linkages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link General Crimes with Weather\n",
    "\n",
    "full_data = pd.merge(full_data,weather,on=[\"Year\",\"Month\"])\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link Specific Crimes with Weather\n",
    "\n",
    "for i in specific_crimes.keys():\n",
    "    specific_crimes[i] = pd.merge(specific_crimes[i],weather,on=[\"Year\",\"Month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Export and Tag Specific Crime Datasets\n",
    "\n",
    "def specific_exporter(dataframe,title):\n",
    "    dataframe[\"Crime Type\"] = title\n",
    "    dataframe = dataframe.sort_values(by=[\"Year\",\"Month\"])\n",
    "\n",
    "    return dataframe.to_csv(f\"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset/{title}/{i}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in specific_crimes.keys():\n",
    "    specific_crimes.copy()[i] = specific_exporter(specific_crimes[i],i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Modules\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics  \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from matplotlib import pyplot as pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from xgboost import XGBRegressor\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerastuner import Hyperband\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#warnings.simplefilter(action='ignore', category=all)\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "full_data = pd.read_csv(\"/home/uthlakanyana/Dropbox/Dissertation Code/Comprehensive Data - 5.csv\")\n",
    "\n",
    "# Directory Navigation\n",
    "d = os.getcwd()\n",
    "print(f'Current: {d}')\n",
    "file_location = \"/home/uthlakanyana/Dropbox/Dissertation Code/Specific Crime Dataset\"\n",
    "os.chdir(file_location)\n",
    "\n",
    "# Obtain Directory Items\n",
    "locations = [file_location+\"/\"+str(f) for f in os.listdir()]\n",
    "    \n",
    "# Obtain the Files Locations in Each Directory\n",
    "dataframes = []\n",
    "for l in locations:\n",
    "    os.chdir(l)\n",
    "    dataframes.extend([l+\"/\"+i for i in os.listdir(l)])\n",
    "\n",
    "# Load Specific Crime Data\n",
    "\n",
    "specific_crimes = {}\n",
    "\n",
    "for d in dataframes:\n",
    "    specific_crimes[d.rsplit(\"/\")[-1].rsplit(\".\")[0]] = pd.read_csv(d).sort_values(by=[\"Year\",\"Month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in full_data.columns:\n",
    "    print(i,len(full_data[full_data[i].isna()==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in specific_crimes:\n",
    "    print(i,len(specific_crimes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Rename Columns\n",
    "\n",
    "def renamer(dataframe):\n",
    "\n",
    "    dataframe = dataframe\n",
    "    renamed_columns = {\"Working households (thousands)\":\"WH\",\"Working households (per cent)\":\"%WH\",\"Mixed households (thousands)\":\"MH\",\n",
    "                       \"Mixed households (per cent)\":\"%MH\",\"Workless households (thousands)\":\"WLH\",\"Workless households (per cent)\":\"%WLH\",\n",
    "                       \"bookmark_count\":\"BKMRK\",\"favorite_count\":\"FVRT\",\"retweet_count\":\"RTWT\",\"reply_count\":\"RPLY\",\"average_sentiment\":\"AVG_SENT\",\n",
    "                       \"Total Population\":\"POP\",\"Area (Hectares)\":\"HECT\",\"All households\":\"LSOA_HHOLDS\",\"Couple household with dependent children\":\"CPHHDC\",\n",
    "                       \"Couple household without dependent children\":\"CPHHWDC\",\"Lone parent household\":\"LPH\",\"One person household\":\"OPH\",\n",
    "                       \"% Couple household with dependent children\":\"%CPHHDC\",\"% Couple household without dependent children\":\"%CPHHWDC\",\n",
    "                       \"% Lone parent household\":\"%LPH\",\"Households with at least one person aged 16 or over with English as a main language\":\"HHEML\",\n",
    "                       \"Owned outright\":\"OO\",\"Owned with a mortgage or loan\":\"OWML\",\"Social rented\":\"SR\",\"Private rented\":\"PR\",\"Owned outright (%)\":\"%OO\",\n",
    "                       \"Owned with a mortgage or loan (%)\":\"%OWML\",\"Social rented (%)\":\"%SR\",\"Private rented (%)\":\"%PR\",\n",
    "                       \"% of households with no adults in employment: With dependent children\":\"%HWNAEWC\",\"% 0-1 (poor access)\":\"%PA\",\n",
    "                       \"Total Number of Families Claiming Benefit\":\"NFCB\",\"Mean Annual Household Income estimate (£)\":\"MEAN_AIE\",\n",
    "                       \"Median Annual Household Income estimate (£)\":\"MEDIAN_AIE\"}\n",
    "    \n",
    "    dataframe = dataframe.rename(columns = renamed_columns)\n",
    "    #dataframe = dataframe.drop(columns=[\"Area\",\"LSOA_HHOLDS\"])\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call renamer function\n",
    "\n",
    "full_data = renamer(full_data)\n",
    "\n",
    "for i in specific_crimes.keys():\n",
    "    specific_crimes[i] = renamer(specific_crimes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Variables in \n",
    "\"\"\"\n",
    "\n",
    "full_data.isna().any()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Plots\n",
    "\n",
    "\"\"\"\n",
    "sns.set_style('dark')\n",
    "\n",
    "Correlazioni = full_data\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "corr = Correlazioni.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool_))\n",
    "cut_off = 0.5  # only show cells with abs(correlation) at least this value\n",
    "extreme_1 = 0.75  # show with a star\n",
    "extreme_2 = 0.85  # show with a second star\n",
    "extreme_3 = 0.90  # show with a third star\n",
    "mask |= np.abs(corr) < cut_off\n",
    "corr = corr[~mask]  # fill in NaN in the non-desired cells\n",
    "\n",
    "remove_empty_rows_and_cols = True\n",
    "if remove_empty_rows_and_cols:\n",
    "    wanted_cols = np.flatnonzero(np.count_nonzero(~mask, axis=1))\n",
    "    wanted_rows = np.flatnonzero(np.count_nonzero(~mask, axis=0))\n",
    "    corr = corr.iloc[wanted_cols, wanted_rows]\n",
    "\n",
    "annot = [[f\"{val:.1g}\"\n",
    "          + ('' if abs(val) < extreme_1 else '\\n★')  # add one star if abs(val) >= extreme_1\n",
    "          + ('' if abs(val) < extreme_2 else '★')  # add an extra star if abs(val) >= extreme_2\n",
    "          + ('' if abs(val) < extreme_3 else '★')  # add yet an extra star if abs(val) >= extreme_3\n",
    "          for val in row] for row in corr.to_numpy()]\n",
    "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, fmt='.1g', cmap='coolwarm',annot_kws={'size': 7,\"color\":\"black\"})\n",
    "heatmap.set_title('Significant (Above 50%) Correlations Between Features', fontdict={'fontsize': 18}, pad=16)\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_correlations = full_data.corr().unstack().sort_values().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoplifting = specific_crimes[\"Shoplifting\"][[\"Year\",\"Month\",\"LSOA11NM\",\"Differences\"]]\n",
    "shoplifting['Year'] = pd.to_datetime(shoplifting[['Year', 'Month']].assign(DAY=1))\n",
    "shoplifting = shoplifting.drop(columns=[\"Month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoplifting2 = shoplifting[shoplifting[\"LSOA11NM\"]==\"Westminster 016A\"]\n",
    "shoplifting2 = shoplifting2.reset_index(drop=True)\n",
    "shoplifting2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shoplifting2)):\n",
    "    if shoplifting2[\"Differences\"].iloc[i] == 0:\n",
    "        shoplifting2.at[i,\"Differences\"] = \"Ebb\"\n",
    "    else:\n",
    "        shoplifting2.at[i,\"Differences\"] = \"Flow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoplifting2[\"Crime Tides\"] = shoplifting2[\"Differences\"]\n",
    "shoplifting2 = shoplifting2.drop(columns=[\"Differences\"])\n",
    "shoplifting2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax = sns.catplot(x=\"Year\",y=\"Crime Tides\",data=shoplifting2,hue=\"Crime Tides\",palette=\"flare\",height=5,aspect=2/1,order=[\"Flow\",\"Ebb\"]).set(title='Westminster 016A Shoplifting Trends')\n",
    "#ax.set_xticklabels(rotation=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax = sns.catplot(x=\"Year\",data=shoplifting2,kind=\"count\",hue=\"Crime Tides\",palette=\"flare\",height=5,aspect=2/1).set(title='Westminster 016A Shoplifting Trends')\n",
    "#ax.set_xticklabels(rotation=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax = sns.barplot(data=shoplifting2, x=\"Year\", y=\"Differences\",color=\"darkred\",hue=\"Differences\")\n",
    "x_dates = shoplifting2['Year'].dt.strftime('%Y-%m').sort_values().unique()\n",
    "ax.set_xticklabels(labels=x_dates, rotation=45)\n",
    "ax.set_ylim(0,1,auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Treatment/Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random State\n",
    "\n",
    "random_state = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Specific Crime Into One Dataset\n",
    "\n",
    "specific_crimes = pd.concat(specific_crimes.values())\n",
    "specific_crimes = specific_crimes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Full Data in to X and Y Variables\n",
    "\n",
    "full_data = {\"x\": full_data.drop(columns=\"Differences\"),\n",
    "             \"y\":full_data[\"Differences\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Specific Crimes Into X and Y Variables\n",
    "\n",
    "specific_crimes = {\"x\": specific_crimes.drop(columns=\"Differences\"),\n",
    "             \"y\":specific_crimes[\"Differences\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split General Crimes Into Train and Test Splits\n",
    "\n",
    "full_train_x, full_test_x, full_train_y, full_test_y = train_test_split(full_data[\"x\"],full_data[\"y\"],test_size=0.10,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Specific Crimes Into Train and Test Splits\n",
    "\n",
    "specific_train_x, specific_test_x, specific_train_y, specific_test_y = train_test_split(specific_crimes[\"x\"],specific_crimes[\"y\"],test_size=0.10,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Treat Dataframes\n",
    "\n",
    "def treat_dataframes(x):\n",
    "\n",
    "    scaled_features = StandardScaler().fit_transform(x.drop(columns=[\"LSOA11NM\",\"text\",\"LSOA_Name\",\"LSOA_Code\"]).values)\n",
    "    scaled_features_df = pd.DataFrame(scaled_features, index=x.index, columns=x.drop(columns=[\"LSOA11NM\",\"text\",\"LSOA_Name\",\"LSOA_Code\"]).columns)\n",
    "\n",
    "    return scaled_features_df\n",
    "\n",
    "\n",
    "def treat_dataframes(x):\n",
    "    \n",
    "    scale_cols = ['WH', '%WH', 'MH', '%MH', 'WLH', '%WLH',\n",
    "       'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ', 'Area',\n",
    "       'BKMRK', 'FVRT', 'RTWT', 'RPLY', 'AVG_SENT',\n",
    "       'POP', 'HECT', 'LSOA_HHOLDS',\n",
    "       'CPHHDC', 'CPHHWDC', 'LPH', 'OPH', '%CPHHDC', '%CPHHWDC', '%LPH',\n",
    "       'HHEML', 'OO', 'OWML', 'SR', 'PR', '%OO', '%OWML', '%SR', '%PR',\n",
    "       'Median Price', 'Sales', '%HWNAEWC', '%PA', 'NFCB', 'MEAN_AIE',\n",
    "       'MEDIAN_AIE', 'Max_Temp', 'Min_Temp', 'Frost Days', 'Rain', 'Sun']\n",
    "\n",
    "\n",
    "    x = x.drop(columns=[\"text\",\"LSOA_Code\",\"LSOA_Name\",\"Crime Density 2022\"])\n",
    "    encoder = ce.BinaryEncoder(cols=[\"Year\",\"Month\",\"LSOA11NM\"])\n",
    "    x = encoder.fit_transform(x)\n",
    "    \n",
    "    x_index = x.index\n",
    "\n",
    "    cols = x.columns\n",
    "\n",
    "    x.loc[:,scale_cols] = StandardScaler().fit_transform(x[scale_cols])\n",
    "\n",
    "    drops = ['WH', '%WH', 'MH', '%MH', 'WLH', '%WLH',\n",
    "       'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ', 'Area',\n",
    "       'POP', 'HECT', 'LSOA_HHOLDS',\n",
    "       'CPHHDC', 'CPHHWDC', 'LPH', 'OPH', '%CPHHDC', '%CPHHWDC', '%LPH',\n",
    "       'HHEML', 'OO', 'OWML', 'SR', 'PR', '%OO', '%OWML', '%SR', '%PR',\n",
    "       'Median Price', 'Sales', '%HWNAEWC', '%PA', 'NFCB', 'MEAN_AIE',\n",
    "       'MEDIAN_AIE']\n",
    "\n",
    "\n",
    "    x = x.drop(columns=drops)\n",
    "\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_dataframes(x).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Discrete Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Inputs (General)\n",
    "\n",
    "crime_type = \"General\"\n",
    "x = full_train_x\n",
    "y = full_train_y\n",
    "test_x = full_test_x\n",
    "test_y = full_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(specific_train_x[\"Crime Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type = \"Other theft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Inputs (Specific)\n",
    "\n",
    "\n",
    "\n",
    "specific_x = specific_train_x.copy()[specific_train_x.copy()[\"Crime Type\"]==crime_type].drop(columns=\"Crime Type\")\n",
    "index = specific_x.index\n",
    "specific_y = specific_train_y.loc[index]\n",
    "\n",
    "test_specific_x = specific_test_x.copy()[specific_test_x.copy()[\"Crime Type\"]==crime_type].drop(columns=\"Crime Type\")\n",
    "test_index = test_specific_x.index\n",
    "test_specific_y = specific_test_y.loc[test_index]\n",
    "\n",
    "x = specific_x\n",
    "y = specific_y\n",
    "test_x = test_specific_x\n",
    "test_y = test_specific_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Dimensions and Crime Type\n",
    "print(crime_type)\n",
    "\n",
    "print(\"Training Dimensions are\",len(treat_dataframes(x).columns), \"x\", len(x))\n",
    "print(\"Test Dimensions are\",len(treat_dataframes(x).columns), \"x\", len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Hyperparameters\n",
    "\n",
    "def support_vector_machines_params(x,y):\n",
    "\n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "    f1 = make_scorer(f1_score,average='macro')\n",
    "\n",
    "    #param_distributions = {\n",
    "    #                       \"bootstrap\": [True, False],\n",
    "    #                       \"bootstrap_features\": [True, False],\n",
    "    #                       \"base_estimator__C\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "    #                       \"base_estimator__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    #                       \"base_estimator__degree\": [1,2,3,4,5,6,7,8,9,10],\n",
    "    #                       \"base_estimator__gamma\": [\"scale\",\"auto\"],\n",
    "    #                       \"base_estimator__coef0\": [0,1,2,3,4,5],\n",
    "    #                       \"base_estimator__shrinking\": [True,False],\n",
    "    #                       \"base_estimator__probability\": [True,False],\n",
    "    #                       }\n",
    "\n",
    "    param_distributions = {                        \n",
    "                           \"C\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                           \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "                           \"degree\": [1,2,3,4,5,6,7,8,9,10],\n",
    "                           \"gamma\": [\"scale\",\"auto\"],\n",
    "                           \"coef0\": [0,1,2,3,4,5],\n",
    "                           \"shrinking\": [True,False],\n",
    "                           \"probability\": [True,False],\n",
    "                           }\n",
    "    \n",
    "    \n",
    "\n",
    "    smote = SMOTE(\n",
    "                    sampling_strategy='minority',\n",
    "                    random_state=random_state, k_neighbors=5,)\n",
    "    x, y = smote.fit_resample(x, y)\n",
    "\n",
    "    search = HalvingRandomSearchCV(svm.SVC(), param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "\n",
    "    #search = HalvingRandomSearchCV(BalancedBaggingClassifier(base_estimator=svm.SVC(),random_state=random_state,sampling_strategy=\"not majority\",n_estimators=150,sampler=SMOTE()),param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "    #search.best_params_.pop(\"bootstrap\")\n",
    "    #search.best_params_.pop(\"bootstrap_features\")\n",
    "    found = search.best_params_.copy()\n",
    "    \n",
    "    #found[\"shrinking\"] = found.pop(\"base_estimator__shrinking\")\n",
    "    #found[\"probability\"] = found.pop(\"base_estimator__probability\")\n",
    "    #found[\"kernel\"] = found.pop(\"base_estimator__kernel\")\n",
    "    #found[\"gamma\"] = found.pop(\"base_estimator__gamma\")\n",
    "    #found[\"degree\"] = found.pop(\"base_estimator__degree\")\n",
    "    #found[\"coef0\"] = found.pop(\"base_estimator__coef0\")\n",
    "    #found[\"C\"] = found.pop(\"base_estimator__C\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return found\n",
    "# Support Vector Machine Fitting\n",
    "\n",
    "def support_vector_machines_fitting(x,y,params):\n",
    "\n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "\n",
    "    clf = svm.SVC(**params)\n",
    "\n",
    "    return clf.fit(x,y)\n",
    "# Support Vector Machine Prediction\n",
    "\n",
    "def support_vector_machines_prediction(model,x):\n",
    "\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hyperparameters\n",
    "\n",
    "svm_hyperparams = support_vector_machines_params(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show HyperParameters\n",
    "\n",
    "svm_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Calls\n",
    "\n",
    "svm_fitting = support_vector_machines_fitting(x,y,svm_hyperparams)\n",
    "svm_prediction = treat_dataframes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Test\n",
    "\n",
    "svm_predict_test = support_vector_machines_prediction(svm_fitting,svm_prediction)\n",
    "svm_predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show SVM Accuracy\n",
    "\n",
    "accuracy_score(y_pred=svm_predict_test,y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate F1 Score\n",
    "\n",
    "f1_score(svm_predict_test,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Hyperparameters\n",
    "\n",
    "def ex_gee_boost_params(x,y):\n",
    "\n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "    f1 = make_scorer(f1_score,average='macro')\n",
    "\n",
    "\n",
    "    #param_distributions = {\n",
    "    #                       \"bootstrap\": [True, False],\n",
    "    #                       \"bootstrap_features\": [True, False],\n",
    "    #                       \"base_estimator__n_estimators\":[12,30,50,100,125,150,175,200,225,250,275,300,325,350,375,400,425,450,475,500],\n",
    "    #                       \"base_estimator__objective\":[\"binary:logistic\"],\n",
    "    #                       \"base_estimator__learning_rate\":[0.001,0.003, 0.006,0.009,0.01,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.15,0.2],\n",
    "    #                       \"base_estimator__max_depth\":[1,2,3,4,5],\n",
    "    #                       \"base_estimator__gamma\":[1,2,3,4,5],\n",
    "    #                       \"base_estimator__colsample_bylevel\":[0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,1],\n",
    "    #                       \"base_estimator__colsample_bytree\":[0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,1],\n",
    "    #                       \"base_estimator__base_score\":[0.25,0.5,0.6]}\n",
    "    param_distributions = {\n",
    "                           \"n_estimators\":[12,30,50,100,125,150,175,200,225,250,275,300,325,350,375,400,425,450,475,500],\n",
    "                           \"objective\":[\"binary:logistic\"],\n",
    "                           \"learning_rate\":[0.001,0.003, 0.006,0.009,0.01,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.15,0.2],\n",
    "                           \"max_depth\":[1,2,3,4,5],\n",
    "                           \"gamma\":[1,2,3,4,5],\n",
    "                           \"colsample_bylevel\":[0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,1],\n",
    "                           \"colsample_bytree\":[0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,1],\n",
    "                           \"base_score\":[0.25,0.5,0.6]}\n",
    "\n",
    "    smote = SMOTE(\n",
    "                sampling_strategy='minority',\n",
    "                random_state=random_state, k_neighbors=5,)\n",
    "\n",
    "    x, y = smote.fit_resample(x, y)\n",
    "    \n",
    "    \n",
    "    search = HalvingRandomSearchCV(xgb.XGBClassifier(), param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "\n",
    "    #search = HalvingRandomSearchCV(BalancedBaggingClassifier(base_estimator=xgb.XGBClassifier(),random_state=random_state,sampling_strategy=\"not majority\",n_estimators=150,sampler=SMOTE()), param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "    found = search.best_params_.copy()\n",
    "    \n",
    "    #found.pop(\"bootstrap\")\n",
    "    #found.pop(\"bootstrap_features\")\n",
    "    #found[\"n_estimators\"] = found.pop(\"base_estimator__n_estimators\")\n",
    "    #found[\"objective\"] = found.pop(\"base_estimator__objective\")\n",
    "    #found[\"learning_rate\"] = found.pop(\"base_estimator__learning_rate\")\n",
    "    #found[\"max_depth\"] = found.pop(\"base_estimator__max_depth\")\n",
    "    #found[\"gamma\"] = found.pop(\"base_estimator__gamma\")\n",
    "    #found[\"colsample_bylevel\"] = found.pop(\"base_estimator__colsample_bylevel\")\n",
    "    #found[\"colsample_bytree\"] = found.pop(\"base_estimator__colsample_bytree\")\n",
    "    #found[\"base_score\"] = found.pop(\"base_estimator__base_score\")\n",
    "    \n",
    "    return found\n",
    "# XGBoost Model Predictions\n",
    "\n",
    "def ex_gee_boost_fitting(x,y,params):\n",
    "\n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "\n",
    "    xgbc = xgb.XGBClassifier(**params)\n",
    "    print(xgbc)\n",
    "    #xgbc.fit(x,y)\n",
    "    \n",
    "    \n",
    "    return xgbc.fit(x,y)\n",
    "\n",
    "def ex_gee_boost_prediction(model,x):\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hyperparameters\n",
    "\n",
    "xgb_hyperparams = ex_gee_boost_params(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Hyperparameters\n",
    "\n",
    "xgb_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Calls\n",
    "\n",
    "xgb_fitting = ex_gee_boost_fitting(x,y,ex_gee_boost_params(x,y))\n",
    "xgb_prediction = treat_dataframes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Test\n",
    "\n",
    "xgb_predict_test = ex_gee_boost_prediction(xgb_fitting,xgb_prediction)\n",
    "xgb_predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Accuracy\n",
    "\n",
    "accuracy_score(y_pred=xgb_predict_test,y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate F1 Score\n",
    "\n",
    "f1_score(xgb_predict_test,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Hyperparameters\n",
    "\n",
    "def multi_layer_perceptron_params(x,y):\n",
    "    \n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "    f1 = make_scorer(f1_score,average='macro')\n",
    "\n",
    "\n",
    "    #param_distributions = {\n",
    "    #                       \"bootstrap\": [True, False],\n",
    "    #                       \"bootstrap_features\": [True, False],\n",
    "    #                       \"base_estimator__hidden_layer_sizes\":[(5),(5,5),(5,5,5),(10),(20),(30),(40),(10,10),(20,20),(30,30),(40,40),(10,10,10),(20,20,20),(30,30,30),(40,40,40),(10,30,10),(20,40,10),(10,40,20),(30,10,30),(100),(200),(100,100),(100,200),(200,100),(300,200,100)],\n",
    "    #                       \"base_estimator__activation\":[\"identity\",\"logistic\",\"tanh\",\"relu\"],\n",
    "    #                       \"base_estimator__solver\":[\"lbfgs\",\"sgd\",\"adam\"],\n",
    "    #                       \"base_estimator__alpha\":[0.0001,0.0005,0.001,0.005,0.01,0.05],\n",
    "    #                       \"base_estimator__batch_size\": [50,100,150,200,250,300,350],\n",
    "    #                       \"base_estimator__learning_rate\": [\"constant\",\"invscaling\",\"adaptive\"],\n",
    "    #                       \"base_estimator__max_iter\":[200,300,500]\n",
    "    #                       }\n",
    "\n",
    "    param_distributions = {\n",
    "                           \"hidden_layer_sizes\":[(5),(5,5),(5,5,5),(10),(20),(30),(40),(10,10),(20,20),(30,30),(40,40),(10,10,10),(20,20,20),(30,30,30),(40,40,40),(10,30,10),(20,40,10),(10,40,20),(30,10,30),(100),(200),(100,100),(100,200),(200,100),(300,200,100)],\n",
    "                           \"activation\":[\"identity\",\"logistic\",\"tanh\",\"relu\"],\n",
    "                           \"solver\":[\"lbfgs\",\"sgd\",\"adam\"],\n",
    "                           \"alpha\":[0.0001,0.0005,0.001,0.005,0.01,0.05],\n",
    "                           \"batch_size\": [50,100,150,200,250,300,350],\n",
    "                           \"learning_rate\": [\"constant\",\"invscaling\",\"adaptive\"],\n",
    "                           \"max_iter\":[200,300,500]\n",
    "                           }\n",
    "\n",
    "    smote = SMOTE(\n",
    "                  sampling_strategy='minority',\n",
    "                  random_state=random_state, k_neighbors=5,)\n",
    "    \n",
    "    x, y = smote.fit_resample(x, y)\n",
    "\n",
    "    #search = HalvingRandomSearchCV(BalancedBaggingClassifier(base_estimator=MLPClassifier(),random_state=random_state,sampling_strategy=\"not majority\",n_estimators=150,sampler=SMOTE()), param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "    search = HalvingRandomSearchCV(MLPClassifier(), param_distributions,random_state=random_state,scoring=f1).fit(x, y)\n",
    "    found = search.best_params_\n",
    "\n",
    "    #found.pop(\"bootstrap\")\n",
    "    #found.pop(\"bootstrap_features\")\n",
    "\n",
    "    #found[\"hidden_layer_sizes\"] = found.pop(\"base_estimator__hidden_layer_sizes\")\n",
    "    #found[\"activation\"] = found.pop(\"base_estimator__activation\")\n",
    "    #found[\"solver\"] = found.pop(\"base_estimator__solver\")\n",
    "    #found[\"alpha\"] = found.pop(\"base_estimator__alpha\")\n",
    "    #found[\"batch_size\"] = found.pop(\"base_estimator__batch_size\")\n",
    "    #found[\"learning_rate\"] = found.pop(\"base_estimator__learning_rate\")\n",
    "    #found[\"max_iter\"] = found.pop(\"base_estimator__max_iter\")\n",
    "\n",
    "    return found\n",
    "# Neural Network Fitting\n",
    "\n",
    "def multi_layer_perceptron_fitting(x,y,params):\n",
    "    \n",
    "    x = treat_dataframes(x)\n",
    "    y = y\n",
    "\n",
    "    clf = MLPClassifier(**params)\n",
    "\n",
    "    return clf.fit(x,y)\n",
    "# Neural Network Predict\n",
    "\n",
    "def multi_layer_perceptron_predict(model,x):\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hyperparameters\n",
    "\n",
    "mpl_hyperparams = multi_layer_perceptron_params(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Hyperparameters\n",
    "\n",
    "mpl_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Calls\n",
    "\n",
    "mpl_fitting = multi_layer_perceptron_fitting(x,y,mpl_hyperparams)\n",
    "mpl_prediction = treat_dataframes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Test\n",
    "\n",
    "mpl_predict_test = multi_layer_perceptron_predict(mpl_fitting,mpl_prediction)\n",
    "mpl_predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Accuracy\n",
    "\n",
    "accuracy_score(y_pred=mpl_predict_test,y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate F1 Score\n",
    "\n",
    "f1_score(mpl_predict_test,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Ensemble Prediction\n",
    "\n",
    "ensemble = [i/3 for i in (mpl_predict_test + svm_predict_test + xgb_predict_test)]\n",
    "\n",
    "for i in range(len(ensemble)):\n",
    "    if ensemble[i] < 0.6:\n",
    "        ensemble[i] = 0\n",
    "    else:\n",
    "        ensemble[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC F1 Scores\n",
    "\n",
    "print(f\" SVC Accuracy Score:{round(accuracy_score(y_pred=svm_predict_test,y_true=test_y),2)}\",f\"\\n SVC F1 Score {round(f1_score(y_pred=svm_predict_test,y_true=test_y),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Scores\n",
    "\n",
    "print(f\" XGB Accuracy Score:{round(accuracy_score(y_pred=xgb_predict_test,y_true=test_y),2)}\",f\"\\n XGB F1 Score {round(f1_score(y_pred=xgb_predict_test,y_true=test_y),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPL F1 Score\n",
    "\n",
    "print(f\" MPL Accuracy Score:{round(accuracy_score(y_pred=mpl_predict_test,y_true=test_y),2)}\",f\"\\n MPL F1 Score {round(f1_score(y_pred=mpl_predict_test,y_true=test_y),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ensemble Scores\n",
    "\n",
    "print(f\" Ensemble Accuracy Score:{round(accuracy_score(y_pred=ensemble,y_true=test_y),2)}\",f\"\\n Ensemble F1 Score {round(f1_score(y_pred=ensemble,y_true=test_y),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange Datasets for Collective Scoring\n",
    "\n",
    "datasets = {\"Full Data\":[full_train_x,full_train_y,full_test_x,full_test_y],\n",
    "            \"Anti-social behaviour\":[],\n",
    "            \"Theft from the person\":[],\n",
    "            \"Possession of weapons\":[],\n",
    "            \"Other crime\":[],\n",
    "            \"Violence and sexual offences\":[],\n",
    "            \"Bicycle theft\":[],\n",
    "            \"Criminal damage and arson\":[],\n",
    "            \"Robbery\":[],\n",
    "            \"Public order\":[],\n",
    "            \"Burglary\":[],\n",
    "            \"Vehicle crime\":[],\n",
    "            \"Drugs\":[],\n",
    "            \"Shoplifting\":[],\n",
    "            \"Other theft\":[]\n",
    "            }\n",
    "\n",
    "for i in datasets.keys():\n",
    "    if i != \"Full Data\":\n",
    "        datasets[i].append(specific_train_x.copy()[specific_train_x.copy()[\"Crime Type\"]==i].drop(columns=\"Crime Type\"))\n",
    "        index = datasets[i][0].index\n",
    "        datasets[i].append(specific_train_y.copy().loc[index])\n",
    "        datasets[i].append(specific_test_x.copy()[specific_test_x.copy()[\"Crime Type\"]==i].drop(columns=\"Crime Type\"))\n",
    "        test_index = datasets[i][2].index\n",
    "        datasets[i].append(specific_test_y.loc[test_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Functions in Loop for 50 Features\n",
    "\n",
    "datasets = {\"Full Data\":[full_train_x,full_train_y,full_test_x,full_test_y],\n",
    "            \"Possession of weapons\":[],\n",
    "            }\n",
    "for i in datasets.keys():\n",
    "    if i != \"Full Data\":\n",
    "        datasets[i].append(specific_train_x.copy()[specific_train_x.copy()[\"Crime Type\"]==i].drop(columns=\"Crime Type\"))\n",
    "        index = datasets[i][0].index\n",
    "        datasets[i].append(specific_train_y.copy().loc[index])\n",
    "        datasets[i].append(specific_test_x.copy()[specific_test_x.copy()[\"Crime Type\"]==i].drop(columns=\"Crime Type\"))\n",
    "        test_index = datasets[i][2].index\n",
    "        datasets[i].append(specific_test_y.loc[test_index])\n",
    "\n",
    "\n",
    "\n",
    "scoring = dict.fromkeys(datasets)\n",
    "\n",
    "def treat_dataframes(x):\n",
    "    \n",
    "    scale_cols = ['WH', '%WH', 'MH', '%MH', 'WLH', '%WLH',\n",
    "       'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ', 'Area',\n",
    "       'BKMRK', 'FVRT', 'RTWT', 'RPLY', 'AVG_SENT',\n",
    "       'POP', 'HECT', 'LSOA_HHOLDS',\n",
    "       'CPHHDC', 'CPHHWDC', 'LPH', 'OPH', '%CPHHDC', '%CPHHWDC', '%LPH',\n",
    "       'HHEML', 'OO', 'OWML', 'SR', 'PR', '%OO', '%OWML', '%SR', '%PR',\n",
    "       'Median Price', 'Sales', '%HWNAEWC', '%PA', 'NFCB', 'MEAN_AIE',\n",
    "       'MEDIAN_AIE', 'Max_Temp', 'Min_Temp', 'Frost Days', 'Rain', 'Sun']\n",
    "\n",
    "\n",
    "    x = x.drop(columns=[\"text\",\"LSOA_Code\",\"LSOA_Name\",\"Crime Density 2022\"])\n",
    "    encoder = ce.BinaryEncoder(cols=[\"Year\",\"Month\",\"LSOA11NM\"])\n",
    "    x = encoder.fit_transform(x)\n",
    "    \n",
    "    x_index = x.index\n",
    "\n",
    "    cols = x.columns\n",
    "\n",
    "    x.loc[:,scale_cols] = StandardScaler().fit_transform(x[scale_cols])\n",
    "\n",
    "    #x = x.loc[:,[\"AVG_SENT\",\"BKMRK\",\"RTWT\",\"RPLY\",\"Max_Temp\",\"Min_Temp\",\"Frost Days\",\"Rain\",\"Sun\"]]\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "for i in datasets.keys():\n",
    "\n",
    "    scoring[i] = dict()\n",
    "\n",
    "    x = datasets[i][0]\n",
    "    y = datasets[i][1]\n",
    "    test_x = datasets[i][2]\n",
    "    test_y = datasets[i][3]\n",
    "\n",
    "    # Call SVMs\n",
    "    svm_hyperparams = support_vector_machines_params(x,y)\n",
    "    svm_fitting = support_vector_machines_fitting(x,y,svm_hyperparams)\n",
    "    svm_prediction = treat_dataframes(test_x)\n",
    "    svm_predict_test = support_vector_machines_prediction(svm_fitting,svm_prediction)\n",
    "\n",
    "    # Call XGBs\n",
    "    xgb_hyperparams = ex_gee_boost_params(x,y)\n",
    "    xgb_fitting = ex_gee_boost_fitting(x,y,ex_gee_boost_fitting(x,y,ex_gee_boost_params(x,y)))#\n",
    "    xgb_prediction = treat_dataframes(test_x)\n",
    "    xgb_predict_test = ex_gee_boost_prediction(xgb_fitting,xgb_prediction)\n",
    "\n",
    "    # Call MLPs\n",
    "    mpl_hyperparams = multi_layer_perceptron_params(x,y)\n",
    "    mpl_fitting = multi_layer_perceptron_fitting(x,y,mpl_hyperparams)\n",
    "    mpl_prediction = treat_dataframes(test_x)\n",
    "    mpl_predict_test = multi_layer_perceptron_predict(mpl_fitting,mpl_prediction)\n",
    "\n",
    "    ensemble = [i/3 for i in (mpl_predict_test + svm_predict_test + xgb_predict_test)]\n",
    "\n",
    "    for i in range(len(ensemble)):\n",
    "        if ensemble[i] < 0.6:\n",
    "            ensemble[i] = 0\n",
    "        else:\n",
    "            ensemble[i] = 1\n",
    "\n",
    "    print(i)\n",
    "    print(\"------------------------------\")\n",
    "    print(f\" SVC Accuracy Score:{round(accuracy_score(y_pred=svm_predict_test,y_true=test_y),2)}\",f\"\\n SVC F1 Score {round(f1_score(y_pred=svm_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" XGB Accuracy Score:{round(accuracy_score(y_pred=xgb_predict_test,y_true=test_y),2)}\",f\"\\n XGB F1 Score {round(f1_score(y_pred=xgb_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" MPL Accuracy Score:{round(accuracy_score(y_pred=mpl_predict_test,y_true=test_y),2)}\",f\"\\n MPL F1 Score {round(f1_score(y_pred=mpl_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" Ensemble Accuracy Score:{round(accuracy_score(y_pred=ensemble,y_true=test_y),2)}\",f\"\\n Ensemble F1 Score {round(f1_score(y_pred=ensemble,y_true=test_y),2)}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    \n",
    "    scoring[i][\"SVM\"] = dict()\n",
    "    scoring[i][\"SVM\"][\"Accuracy\"] = round(accuracy_score(y_pred=svm_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"SVM\"][\"F1 Score\"] = round(f1_score(y_pred=svm_predict_test,y_true=test_y),2)\n",
    "    \n",
    "    scoring[i][\"XGB\"] = dict()\n",
    "    scoring[i][\"XGB\"][\"Accuracy\"] = round(accuracy_score(y_pred=xgb_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"XGB\"][\"F1 Score\"] = round(f1_score(y_pred=xgb_predict_test,y_true=test_y),2)\n",
    "\n",
    "    scoring[i][\"MPL\"] = dict()\n",
    "    scoring[i][\"MPL\"][\"Accuracy\"] = round(accuracy_score(y_pred=mpl_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"MPL\"][\"F1 Score\"] = round(f1_score(y_pred=mpl_predict_test,y_true=test_y),2)\n",
    "\n",
    "    scoring[i][\"Ensemble\"] = dict()\n",
    "    scoring[i][\"Ensemble\"][\"Accuracy\"] = round(accuracy_score(y_pred=ensemble,y_true=test_y),2)\n",
    "    scoring[i][\"Ensemble\"][\"F1 Score\"] = round(f1_score(y_pred=ensemble,y_true=test_y),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Functions in Loop for 9 Features\n",
    "\n",
    "scoring = dict.fromkeys(datasets)\n",
    "\n",
    "def treat_dataframes(x):\n",
    "    \n",
    "    scale_cols = ['WH', '%WH', 'MH', '%MH', 'WLH', '%WLH',\n",
    "       'COMESTRES', 'POPDEN', 'HHOLDS', 'AVHHOLDSZ', 'Area',\n",
    "       'BKMRK', 'FVRT', 'RTWT', 'RPLY', 'AVG_SENT',\n",
    "       'POP', 'HECT', 'LSOA_HHOLDS',\n",
    "       'CPHHDC', 'CPHHWDC', 'LPH', 'OPH', '%CPHHDC', '%CPHHWDC', '%LPH',\n",
    "       'HHEML', 'OO', 'OWML', 'SR', 'PR', '%OO', '%OWML', '%SR', '%PR',\n",
    "       'Median Price', 'Sales', '%HWNAEWC', '%PA', 'NFCB', 'MEAN_AIE',\n",
    "       'MEDIAN_AIE', 'Max_Temp', 'Min_Temp', 'Frost Days', 'Rain', 'Sun']\n",
    "\n",
    "\n",
    "    x = x.drop(columns=[\"text\",\"LSOA_Code\",\"LSOA_Name\",\"Crime Density 2022\"])\n",
    "    encoder = ce.BinaryEncoder(cols=[\"Year\",\"Month\",\"LSOA11NM\"])\n",
    "    x = encoder.fit_transform(x)\n",
    "    \n",
    "    x_index = x.index\n",
    "\n",
    "    cols = x.columns\n",
    "\n",
    "    x.loc[:,scale_cols] = StandardScaler().fit_transform(x[scale_cols])\n",
    "\n",
    "    x = x.loc[:,[\"AVG_SENT\",\"BKMRK\",\"RTWT\",\"RPLY\",\"Max_Temp\",\"Min_Temp\",\"Frost Days\",\"Rain\",\"Sun\"]]\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "for i in datasets.keys():\n",
    "\n",
    "    scoring[i] = dict()\n",
    "\n",
    "    x = datasets[i][0]\n",
    "    y = datasets[i][1]\n",
    "    test_x = datasets[i][2]\n",
    "    test_y = datasets[i][3]\n",
    "\n",
    "    # Call SVMs\n",
    "    svm_hyperparams = support_vector_machines_params(x,y)\n",
    "    svm_fitting = support_vector_machines_fitting(x,y,svm_hyperparams)\n",
    "    svm_prediction = treat_dataframes(test_x)\n",
    "    svm_predict_test = support_vector_machines_prediction(svm_fitting,svm_prediction)\n",
    "\n",
    "    # Call XGBs\n",
    "    xgb_hyperparams = ex_gee_boost_params(x,y)\n",
    "    xgb_fitting = ex_gee_boost_fitting(x,y,ex_gee_boost_fitting(x,y,ex_gee_boost_params(x,y)))\n",
    "    xgb_prediction = treat_dataframes(test_x)\n",
    "    xgb_predict_test = ex_gee_boost_prediction(xgb_fitting,xgb_prediction)\n",
    "\n",
    "    # Call MLPs\n",
    "    mpl_hyperparams = multi_layer_perceptron_params(x,y)\n",
    "    mpl_fitting = multi_layer_perceptron_fitting(x,y,mpl_hyperparams)\n",
    "    mpl_prediction = treat_dataframes(test_x)\n",
    "    mpl_predict_test = multi_layer_perceptron_predict(mpl_fitting,mpl_prediction)\n",
    "\n",
    "    ensemble = [i/3 for i in (mpl_predict_test + svm_predict_test + xgb_predict_test)]\n",
    "\n",
    "    for i in range(len(ensemble)):\n",
    "        if ensemble[i] < 0.6:\n",
    "            ensemble[i] = 0\n",
    "        else:\n",
    "            ensemble[i] = 1\n",
    "\n",
    "    print(i)\n",
    "    print(\"------------------------------\")\n",
    "    print(f\" SVC Accuracy Score:{round(accuracy_score(y_pred=svm_predict_test,y_true=test_y),2)}\",f\"\\n SVC F1 Score {round(f1_score(y_pred=svm_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" XGB Accuracy Score:{round(accuracy_score(y_pred=xgb_predict_test,y_true=test_y),2)}\",f\"\\n XGB F1 Score {round(f1_score(y_pred=xgb_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" MPL Accuracy Score:{round(accuracy_score(y_pred=mpl_predict_test,y_true=test_y),2)}\",f\"\\n MPL F1 Score {round(f1_score(y_pred=mpl_predict_test,y_true=test_y),2)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\" Ensemble Accuracy Score:{round(accuracy_score(y_pred=ensemble,y_true=test_y),2)}\",f\"\\n Ensemble F1 Score {round(f1_score(y_pred=ensemble,y_true=test_y),2)}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    \n",
    "    scoring[i][\"SVM\"] = dict()\n",
    "    scoring[i][\"SVM\"][\"Accuracy\"] = round(accuracy_score(y_pred=svm_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"SVM\"][\"F1 Score\"] = round(f1_score(y_pred=svm_predict_test,y_true=test_y),2)\n",
    "    \n",
    "    scoring[i][\"XGB\"] = dict()\n",
    "    scoring[i][\"XGB\"][\"Accuracy\"] = round(accuracy_score(y_pred=xgb_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"XGB\"][\"F1 Score\"] = round(f1_score(y_pred=xgb_predict_test,y_true=test_y),2)\n",
    "\n",
    "    scoring[i][\"MPL\"] = dict()\n",
    "    scoring[i][\"MPL\"][\"Accuracy\"] = round(accuracy_score(y_pred=mpl_predict_test,y_true=test_y),2)\n",
    "    scoring[i][\"MPL\"][\"F1 Score\"] = round(f1_score(y_pred=mpl_predict_test,y_true=test_y),2)\n",
    "\n",
    "    scoring[i][\"Ensemble\"] = dict()\n",
    "    scoring[i][\"Ensemble\"][\"Accuracy\"] = round(accuracy_score(y_pred=ensemble,y_true=test_y),2)\n",
    "    scoring[i][\"Ensemble\"][\"F1 Score\"] = round(f1_score(y_pred=ensemble,y_true=test_y),2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
